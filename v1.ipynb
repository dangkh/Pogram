{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a981dcde-34cf-453d-99d6-199397a96e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import random\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8efb0018-fc1c-4fc7-b190-56f26fe0bc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import IterableDataset, Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d64364a-4833-4a00-91f0-645c4dfce442",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample(all_elements, num_sample):\n",
    "    if num_sample > len(all_elements):\n",
    "        return random.sample(all_elements * (num_sample // len(all_elements) + 1), num_sample)\n",
    "    else:\n",
    "        return random.sample(all_elements, num_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d9b1aab-6372-4ddf-9b95-5d8d4f9fc719",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dir = './data/MINDsmall_train'\n",
    "num_words_title = 20\n",
    "use_category = True\n",
    "use_subcategory = True\n",
    "processed_file_path = os.path.join(train_data_dir, f'behaviors_np{4}.tsv')\n",
    "word_embedding_dim = 300\n",
    "glove_embedding_path = './data/glove.840B.300d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff72edd8-ab14-4e12-ad04-364588b54211",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_training_data(seed = 1009, npratio = 4):\n",
    "    random.seed(seed)\n",
    "    behaviors = []\n",
    "    \n",
    "    behavior_file_path = os.path.join(train_data_dir, 'behaviors.tsv')\n",
    "    with open(behavior_file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in tqdm(f):\n",
    "            iid, uid, time, history, imp = line.strip().split('\\t')\n",
    "            impressions = [x.split('-') for x in imp.split(' ')]\n",
    "            pos, neg = [], []\n",
    "            for news_ID, label in impressions:\n",
    "                if label == '0':\n",
    "                    neg.append(news_ID)\n",
    "                elif label == '1':\n",
    "                    pos.append(news_ID)\n",
    "            if len(pos) == 0 or len(neg) == 0:\n",
    "                continue\n",
    "            for pos_id in pos:\n",
    "                neg_candidate = get_sample(neg, npratio)\n",
    "                neg_str = ' '.join(neg_candidate)\n",
    "                new_line = '\\t'.join([iid, uid, time, history, pos_id, neg_str]) + '\\n'\n",
    "                behaviors.append(new_line)\n",
    "\n",
    "    random.shuffle(behaviors)\n",
    "    processed_file_path = os.path.join(train_data_dir, f'behaviors_np{npratio}.tsv')\n",
    "    with open(processed_file_path, 'w') as f:\n",
    "        f.writelines(behaviors)\n",
    "    return len(behaviors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ac3f745-0923-4bcc-921d-fe0cc6c7cc7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "156965it [00:02, 52619.34it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "236344"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepare_training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1e8e49f0-8cb4-4dee-bf45-d03e16779aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_dict(dict, key, value=None):\n",
    "    if key not in dict:\n",
    "        if value is None:\n",
    "            dict[key] = len(dict) + 1\n",
    "        else:\n",
    "            dict[key] = value\n",
    "            \n",
    "def read_news(news_path, mode='train'):\n",
    "    news = {}\n",
    "    category_dict = {}\n",
    "    subcategory_dict = {}\n",
    "    news_index = {}\n",
    "    word_cnt = Counter()\n",
    "    \n",
    "    filter_num = 0\n",
    "    with open(news_path, 'r', encoding='utf-8') as f:\n",
    "        for line in tqdm(f):\n",
    "            splited = line.strip('\\n').split('\\t')\n",
    "            doc_id, category, subcategory, title, abstract, url, _, _ = splited\n",
    "            update_dict(news_index, doc_id)\n",
    "\n",
    "            title = title.lower()\n",
    "            title = word_tokenize(title)\n",
    "            update_dict(news, doc_id, [title, category, subcategory])\n",
    "            if mode == 'train':\n",
    "                if use_category:\n",
    "                    update_dict(category_dict, category)\n",
    "                if use_subcategory:\n",
    "                    update_dict(subcategory_dict, subcategory)\n",
    "                word_cnt.update(title)\n",
    "\n",
    "    if mode == 'train':\n",
    "        word = [k for k, v in word_cnt.items() if v > filter_num]\n",
    "        word_dict = {k: v for k, v in zip(word, range(1, len(word) + 1))}\n",
    "        return news, news_index, category_dict, subcategory_dict, word_dict\n",
    "    elif mode == 'test':\n",
    "        return news, news_index\n",
    "    else:\n",
    "        assert False, 'Wrong mode!'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f58896cf-2a23-4140-bd11-1c15005f8490",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc_input(news, news_index, category_dict, subcategory_dict, word_dict):\n",
    "    news_num = len(news) + 1\n",
    "    news_title = np.zeros((news_num, num_words_title), dtype='int32')\n",
    "    news_category = np.zeros((news_num, 1), dtype='int32') \n",
    "    news_subcategory = np.zeros((news_num, 1), dtype='int32') \n",
    "\n",
    "    for key in tqdm(news):\n",
    "        title, category, subcategory = news[key]\n",
    "        doc_index = news_index[key]\n",
    "\n",
    "        for word_id in range(min(num_words_title, len(title))):\n",
    "            if title[word_id] in word_dict:\n",
    "                news_title[doc_index, word_id] = word_dict[title[word_id]]\n",
    "        \n",
    "        news_category[doc_index, 0] = category_dict[category] if category in category_dict else 0\n",
    "        news_subcategory[doc_index, 0] = subcategory_dict[subcategory] if subcategory in subcategory_dict else 0\n",
    "\n",
    "    return news_title, news_category, news_subcategory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ceefdb12-9687-4af0-8210-6454cefd161b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_matrix(embedding_file_path, word_dict, word_embedding_dim):\n",
    "    embedding_matrix = np.zeros(shape=(len(word_dict) + 1, word_embedding_dim))\n",
    "    have_word = []\n",
    "    if embedding_file_path is not None:\n",
    "        with open(embedding_file_path, 'rb') as f:\n",
    "            while True:\n",
    "                line = f.readline()\n",
    "                if len(line) == 0:\n",
    "                    break\n",
    "                line = line.split()\n",
    "                word = line[0].decode()\n",
    "                if word in word_dict:\n",
    "                    index = word_dict[word]\n",
    "                    tp = [float(x) for x in line[1:]]\n",
    "                    embedding_matrix[index] = np.array(tp)\n",
    "                    have_word.append(word)\n",
    "    return embedding_matrix, have_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d7d26cfe-093e-4ad4-bd1d-45d2d0a84342",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionPooling(nn.Module):\n",
    "    def __init__(self, emb_size, hidden_size):\n",
    "        super(AttentionPooling, self).__init__()\n",
    "        self.att_fc1 = nn.Linear(emb_size, hidden_size)\n",
    "        self.att_fc2 = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x, attn_mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: batch_size, candidate_size, emb_dim\n",
    "            attn_mask: batch_size, candidate_size\n",
    "        Returns:\n",
    "            (shape) batch_size, emb_dim\n",
    "        \"\"\"\n",
    "        e = self.att_fc1(x)\n",
    "        e = nn.Tanh()(e)\n",
    "        alpha = self.att_fc2(e)\n",
    "        alpha = torch.exp(alpha)\n",
    "\n",
    "        if attn_mask is not None:\n",
    "            alpha = alpha * attn_mask.unsqueeze(2)\n",
    "\n",
    "        alpha = alpha / (torch.sum(alpha, dim=1, keepdim=True) + 1e-8)\n",
    "        x = torch.bmm(x.permute(0, 2, 1), alpha).squeeze(dim=-1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, d_k):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "        self.d_k = d_k\n",
    "\n",
    "    def forward(self, Q, K, V, attn_mask=None):\n",
    "        '''\n",
    "            Q: batch_size, n_head, candidate_num, d_k\n",
    "            K: batch_size, n_head, candidate_num, d_k\n",
    "            V: batch_size, n_head, candidate_num, d_v\n",
    "            attn_mask: batch_size, n_head, candidate_num\n",
    "            Return: batch_size, n_head, candidate_num, d_v\n",
    "        '''\n",
    "        scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(self.d_k)\n",
    "        scores = torch.exp(scores)\n",
    "\n",
    "        if attn_mask is not None:\n",
    "            scores = scores * attn_mask.unsqueeze(dim=-2)\n",
    "\n",
    "        attn = scores / (torch.sum(scores, dim=-1, keepdim=True) + 1e-8)\n",
    "        context = torch.matmul(attn, V)\n",
    "        return context\n",
    "\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_k, d_v):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "\n",
    "        self.W_Q = nn.Linear(d_model, d_k * n_heads)\n",
    "        self.W_K = nn.Linear(d_model, d_k * n_heads)\n",
    "        self.W_V = nn.Linear(d_model, d_v * n_heads)\n",
    "\n",
    "        self.scaled_dot_product_attn = ScaledDotProductAttention(self.d_k)\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight, gain=1)\n",
    "\n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        '''\n",
    "            Q: batch_size, candidate_num, d_model\n",
    "            K: batch_size, candidate_num, d_model\n",
    "            V: batch_size, candidate_num, d_model\n",
    "            mask: batch_size, candidate_num\n",
    "        '''\n",
    "        batch_size = Q.shape[0]\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(dim=1).expand(-1, self.n_heads, -1)\n",
    "\n",
    "        q_s = self.W_Q(Q).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        k_s = self.W_K(K).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        v_s = self.W_V(V).view(batch_size, -1, self.n_heads, self.d_v).transpose(1, 2)\n",
    "\n",
    "        context = self.scaled_dot_product_attn(q_s, k_s, v_s, mask)\n",
    "        output = context.transpose(1, 2).contiguous().view(batch_size, -1, self.n_heads * self.d_v)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b1f1de31-d838-4caa-a481-eed704c50759",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsEncoder(nn.Module):\n",
    "    def __init__(self, embedding_matrix, num_category, num_subcategory):\n",
    "        super(NewsEncoder, self).__init__()\n",
    "        self.embedding_matrix = embedding_matrix\n",
    "        self.drop_rate = 0.2\n",
    "        self.num_words_title = 20\n",
    "        self.use_category = True\n",
    "        self.use_subcategory = True\n",
    "        category_emb_dim = 100\n",
    "        news_dim = 400\n",
    "        news_query_vector_dim = 200\n",
    "        word_embedding_dim = 300\n",
    "        self.category_emb = nn.Embedding(num_category + 1, category_emb_dim, padding_idx=0)\n",
    "        self.category_dense = nn.Linear(category_emb_dim, news_dim)\n",
    "        self.subcategory_emb = nn.Embedding(num_subcategory + 1, category_emb_dim, padding_idx=0)\n",
    "        self.subcategory_dense = nn.Linear(category_emb_dim, news_dim)\n",
    "        self.final_attn = AttentionPooling(news_dim, news_query_vector_dim)\n",
    "        self.cnn = nn.Conv1d(\n",
    "            in_channels=word_embedding_dim,\n",
    "            out_channels=news_dim,\n",
    "            kernel_size=3,\n",
    "            padding=1\n",
    "        )\n",
    "        self.attn = AttentionPooling(news_dim, news_query_vector_dim)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        '''\n",
    "            x: batch_size, word_num\n",
    "            mask: batch_size, word_num\n",
    "        '''\n",
    "        title = torch.narrow(x, -1, 0, self.num_words_title).long()\n",
    "        word_vecs = F.dropout(self.embedding_matrix(title),\n",
    "                              p=self.drop_rate,\n",
    "                              training=self.training)\n",
    "        context_word_vecs = self.cnn(word_vecs.transpose(1, 2)).transpose(1, 2)\n",
    "        \n",
    "        title_vecs = self.attn(context_word_vecs, mask)\n",
    "        all_vecs = [title_vecs]\n",
    "\n",
    "        start = self.num_words_title\n",
    "        if self.use_category:\n",
    "            category = torch.narrow(x, -1, start, 1).squeeze(dim=-1).long()\n",
    "            category_vecs = self.category_dense(self.category_emb(category))\n",
    "            all_vecs.append(category_vecs)\n",
    "            start += 1\n",
    "        if self.use_subcategory:\n",
    "            subcategory = torch.narrow(x, -1, start, 1).squeeze(dim=-1).long()\n",
    "            subcategory_vecs = self.subcategory_dense(self.subcategory_emb(subcategory))\n",
    "            all_vecs.append(subcategory_vecs)\n",
    "\n",
    "        if len(all_vecs) == 1:\n",
    "            news_vecs = all_vecs[0]\n",
    "        else:\n",
    "            all_vecs = torch.stack(all_vecs, dim=1)\n",
    "            \n",
    "            news_vecs = self.final_attn(all_vecs)\n",
    "        return news_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5af21979-d05f-42b3-8235-83c1b43d272a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class UserEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UserEncoder, self).__init__()\n",
    "        news_dim = 400\n",
    "        user_query_vector_dim = 200\n",
    "        self.user_log_length = 100\n",
    "        self.user_log_mask = False\n",
    "        self.attn = AttentionPooling(news_dim, user_query_vector_dim)\n",
    "        self.pad_doc = nn.Parameter(torch.empty(1, news_dim).uniform_(-1, 1)).type(torch.FloatTensor)\n",
    "\n",
    "    def forward(self, news_vecs, log_mask=None):\n",
    "        '''\n",
    "            news_vecs: batch_size, history_num, news_dim\n",
    "            log_mask: batch_size, history_num\n",
    "        '''\n",
    "        bz = news_vecs.shape[0]\n",
    "        if self.user_log_mask:\n",
    "            user_vec = self.attn(news_vecs, log_mask)\n",
    "        else:\n",
    "            padding_doc = self.pad_doc.unsqueeze(dim=0).expand(bz, self.user_log_length, -1)\n",
    "            news_vecs = news_vecs * log_mask.unsqueeze(dim=-1) + padding_doc * (1 - log_mask.unsqueeze(dim=-1))\n",
    "            user_vec = self.attn(news_vecs)\n",
    "        return user_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "455ee000-ecf0-4661-973c-bdf52246e33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NAML(torch.nn.Module):\n",
    "    def __init__(self, embedding_matrix, num_category, num_subcategory, **kwargs):\n",
    "        super(NAML, self).__init__()\n",
    "        pretrained_word_embedding = torch.from_numpy(embedding_matrix).float()\n",
    "        word_embedding = nn.Embedding.from_pretrained(pretrained_word_embedding,\n",
    "                                                      freeze=False,\n",
    "                                                      padding_idx=0)\n",
    "\n",
    "        self.news_encoder = NewsEncoder( word_embedding, num_category, num_subcategory)\n",
    "        self.user_encoder = UserEncoder()\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        self.npratio = 4\n",
    "        self.news_dim = 400\n",
    "        self.user_log_length = 100\n",
    "\n",
    "    def forward(self, history, history_mask, candidate, label):\n",
    "        '''\n",
    "            history: batch_size, history_length, num_word_title\n",
    "            history_mask: batch_size, history_length\n",
    "            candidate: batch_size, 1+K, num_word_title\n",
    "            label: batch_size, 1+K\n",
    "        '''\n",
    "        num_words = history.shape[-1]\n",
    "        candidate_news = candidate.reshape(-1, num_words)\n",
    "        candidate_news_vecs = self.news_encoder(candidate_news).reshape(-1, 1 + self.npratio, self.news_dim)\n",
    "        history_news = history.reshape(-1, num_words)\n",
    "        history_news_vecs = self.news_encoder(history_news).reshape(-1, self.user_log_length, self.news_dim)\n",
    "        user_vec = self.user_encoder(history_news_vecs, history_mask)\n",
    "        score = torch.bmm(candidate_news_vecs, user_vec.unsqueeze(dim=-1)).squeeze(dim=-1)\n",
    "        loss = self.loss_fn(score, label)\n",
    "        return loss, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a2eecd1c-3e33-4982-a6cf-ca452c594657",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetTrain(IterableDataset):\n",
    "    def __init__(self, filename, news_index, news_combined):\n",
    "        super(DatasetTrain).__init__()\n",
    "        self.filename = filename\n",
    "        self.news_index = news_index\n",
    "        self.news_combined = news_combined\n",
    "        self.user_log_length = 100\n",
    "        self.npratio = 4\n",
    "\n",
    "    def trans_to_nindex(self, nids):\n",
    "        return [self.news_index[i] if i in self.news_index else 0 for i in nids]\n",
    "\n",
    "    def pad_to_fix_len(self, x, fix_length, padding_front=True, padding_value=0):\n",
    "        if padding_front:\n",
    "            pad_x = [padding_value] * (fix_length - len(x)) + x[-fix_length:]\n",
    "            mask = [0] * (fix_length - len(x)) + [1] * min(fix_length, len(x))\n",
    "        else:\n",
    "            pad_x = x[-fix_length:] + [padding_value] * (fix_length - len(x))\n",
    "            mask = [1] * min(fix_length, len(x)) + [0] * (fix_length - len(x))\n",
    "        return pad_x, np.array(mask, dtype='float32')\n",
    "\n",
    "    def line_mapper(self, line):\n",
    "        line = line.strip().split('\\t')\n",
    "        click_docs = line[3].split()\n",
    "        sess_pos = line[4].split()\n",
    "        sess_neg = line[5].split()\n",
    "\n",
    "        click_docs, log_mask = self.pad_to_fix_len(self.trans_to_nindex(click_docs), self.user_log_length)\n",
    "        user_feature = self.news_combined[click_docs]\n",
    "\n",
    "        pos = self.trans_to_nindex(sess_pos)\n",
    "        neg = self.trans_to_nindex(sess_neg)\n",
    "\n",
    "        label = random.randint(0, self.npratio)\n",
    "        sample_news = neg[:label] + pos + neg[label:]\n",
    "        news_feature = self.news_combined[sample_news]\n",
    "        return user_feature, log_mask, news_feature, label\n",
    "\n",
    "    def __iter__(self):\n",
    "        file_iter = open(self.filename)\n",
    "        return map(self.line_mapper, file_iter)\n",
    "\n",
    "\n",
    "class DatasetTest(DatasetTrain):\n",
    "    def __init__(self, filename, news_index, news_scoring):\n",
    "        super(DatasetTrain).__init__()\n",
    "        self.filename = filename\n",
    "        self.news_index = news_index\n",
    "        self.news_scoring = news_scoring\n",
    "        self.user_log_length = 100\n",
    "\n",
    "    def line_mapper(self, line):\n",
    "        line = line.strip().split('\\t')\n",
    "        click_docs = line[3].split()\n",
    "        click_docs, log_mask = self.pad_to_fix_len(self.trans_to_nindex(click_docs), self.user_log_length)\n",
    "        user_feature = self.news_scoring[click_docs]\n",
    "\n",
    "        candidate_news = self.trans_to_nindex([i.split('-')[0] for i in line[4].split()])\n",
    "        labels = np.array([int(i.split('-')[1]) for i in line[4].split()])\n",
    "        news_feature = self.news_scoring[candidate_news]\n",
    "\n",
    "        return user_feature, log_mask, news_feature, labels\n",
    "\n",
    "    def __iter__(self):\n",
    "        file_iter = open(self.filename)\n",
    "        return map(self.line_mapper, file_iter)\n",
    "\n",
    "\n",
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8a5cdc7e-773c-4dcf-8682-e0fc46d09806",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def acc(y_true, y_hat):\n",
    "    y_hat = torch.argmax(y_hat, dim=-1)\n",
    "    tot = y_true.shape[0]\n",
    "    hit = torch.sum(y_true == y_hat)\n",
    "    return hit.data.float() * 1.0 / tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "71fcf8f3-f1dc-45a0-b6a2-1dc4f56d6a1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51282it [00:05, 9537.24it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 51282/51282 [00:00<00:00, 229934.33it/s]\n"
     ]
    }
   ],
   "source": [
    "# train():\n",
    "news, news_index, category_dict, subcategory_dict, word_dict = read_news(\n",
    "    os.path.join(train_data_dir, 'news.tsv'), mode='train')\n",
    "\n",
    "news_title, news_category, news_subcategory = get_doc_input(\n",
    "    news, news_index, category_dict, subcategory_dict, word_dict)\n",
    "news_combined = np.concatenate([x for x in [news_title, news_category, news_subcategory] if x is not None], axis=-1)\n",
    "embedding_matrix, have_word = load_matrix(glove_embedding_path,\n",
    "                                                word_dict,\n",
    "                                                word_embedding_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "be8c58e1-9fd9-4f70-8a03-006724bb008c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37535"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad4150aa-5b7e-422a-8658-b6ddc16b4fe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51283, 22)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_combined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b1550d5a-44b9-495b-b132-7c8619d09f40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NAML(\n",
       "  (news_encoder): NewsEncoder(\n",
       "    (embedding_matrix): Embedding(12507, 300, padding_idx=0)\n",
       "    (category_emb): Embedding(18, 100, padding_idx=0)\n",
       "    (category_dense): Linear(in_features=100, out_features=400, bias=True)\n",
       "    (subcategory_emb): Embedding(265, 100, padding_idx=0)\n",
       "    (subcategory_dense): Linear(in_features=100, out_features=400, bias=True)\n",
       "    (final_attn): AttentionPooling(\n",
       "      (att_fc1): Linear(in_features=400, out_features=200, bias=True)\n",
       "      (att_fc2): Linear(in_features=200, out_features=1, bias=True)\n",
       "    )\n",
       "    (cnn): Conv1d(300, 400, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (attn): AttentionPooling(\n",
       "      (att_fc1): Linear(in_features=400, out_features=200, bias=True)\n",
       "      (att_fc2): Linear(in_features=200, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (user_encoder): UserEncoder(\n",
       "    (attn): AttentionPooling(\n",
       "      (att_fc1): Linear(in_features=400, out_features=200, bias=True)\n",
       "      (att_fc2): Linear(in_features=200, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (loss_fn): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NAML(embedding_matrix, len(category_dict), len(subcategory_dict))\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0003)\n",
    "model = model.cuda()\n",
    "torch.set_grad_enabled(True)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "25c6ff7f-4a0b-4a43-a2eb-3509e518afd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DatasetTrain(processed_file_path, news_index, news_combined)\n",
    "dataloader = DataLoader(dataset, batch_size=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3fe4dc0a-cbd4-4dba-adbd-f067fa3371e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1847it [03:47,  8.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2566.7004, device='cuda:0') tensor(797.2232, device='cuda:0')\n",
      "EPOCH: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1847it [03:46,  8.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2443.4343, device='cuda:0') tensor(860.4632, device='cuda:0')\n",
      "EPOCH: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1847it [03:49,  8.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2390.1062, device='cuda:0') tensor(887.2489, device='cuda:0')\n",
      "EPOCH: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1847it [03:48,  8.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2348.0330, device='cuda:0') tensor(907.5112, device='cuda:0')\n",
      "EPOCH: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1847it [03:50,  8.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2310.7275, device='cuda:0') tensor(926.6741, device='cuda:0')\n",
      "EPOCH: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1847it [03:46,  8.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2273.4028, device='cuda:0') tensor(940.6909, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for ep in range(6):\n",
    "    loss = 0.0\n",
    "    accuary = 0.0\n",
    "    print(\"EPOCH: \" + str(ep))\n",
    "    for cnt, (log_ids, log_mask, input_ids, targets) in tqdm(enumerate(dataloader)):\n",
    "        log_ids = log_ids.cuda()\n",
    "        log_mask = log_mask.cuda()\n",
    "        input_ids = input_ids.cuda()\n",
    "        targets = targets.cuda()\n",
    "\n",
    "        bz_loss, y_hat = model(log_ids, log_mask, input_ids, targets)\n",
    "        loss += bz_loss.data.float()\n",
    "        accuary += acc(targets, y_hat)\n",
    "        optimizer.zero_grad()\n",
    "        bz_loss.backward()\n",
    "        optimizer.step()\n",
    "        # stop\n",
    "    print(loss, accuary)\n",
    "\n",
    "    #     if rank == 0 and cnt != 0 and cnt % args.save_steps == 0:\n",
    "    #         ckpt_path = os.path.join(args.model_dir, f'epoch-{ep+1}-{cnt}.pt')\n",
    "    #         torch.save(\n",
    "    #             {\n",
    "    #                 'model_state_dict':\n",
    "    #                     {'.'.join(k.split('.')[1:]): v for k, v in model.state_dict().items()}\n",
    "    #                     if is_distributed else model.state_dict(),\n",
    "    #                 'category_dict': category_dict,\n",
    "    #                 'word_dict': word_dict,\n",
    "    #                 'subcategory_dict': subcategory_dict\n",
    "    #             }, ckpt_path)\n",
    "    #         logging.info(f\"Model saved to {ckpt_path}.\")\n",
    "\n",
    "    # logging.info('Training finish.')\n",
    "\n",
    "    # if rank == 0:\n",
    "    #     ckpt_path = os.path.join(args.model_dir, f'epoch-{ep+1}.pt')\n",
    "    #     torch.save(\n",
    "    #         {\n",
    "    #             'model_state_dict':\n",
    "    #                 {'.'.join(k.split('.')[1:]): v for k, v in model.state_dict().items()}\n",
    "    #                 if is_distributed else model.state_dict(),\n",
    "    #             'category_dict': category_dict,\n",
    "    #             'subcategory_dict': subcategory_dict,\n",
    "    #             'word_dict': word_dict,\n",
    "    #         }, ckpt_path)\n",
    "    #     logging.info(f\"Model saved to {ckpt_path}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "07a1e9b1-356e-453c-97ae-160e10c9a982",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_dir = './data/MINDsmall_dev'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "462e0c7c-3857-446c-b4f6-9e53252306a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "42416it [00:04, 10416.95it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 42416/42416 [00:00<00:00, 237593.63it/s]\n"
     ]
    }
   ],
   "source": [
    "model.cuda()\n",
    "\n",
    "model.eval()\n",
    "torch.set_grad_enabled(False)\n",
    "news, news_index = read_news(os.path.join(test_data_dir, 'news.tsv'), mode='test')\n",
    "news_title, news_category, news_subcategory = get_doc_input(\n",
    "    news, news_index, category_dict, subcategory_dict, word_dict)\n",
    "news_combined = np.concatenate([x for x in [news_title, news_category, news_subcategory] if x is not None], axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e5c0e3a4-3988-4887-83cf-c9e655346b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "news_dataset = NewsDataset(news_combined)\n",
    "news_dataloader = DataLoader(news_dataset, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "aff50686-a512-4088-98c7-57945f68a6ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 332/332 [00:00<00:00, 370.61it/s]\n"
     ]
    }
   ],
   "source": [
    "news_scoring = []\n",
    "with torch.no_grad():\n",
    "    for input_ids in tqdm(news_dataloader):\n",
    "        input_ids = input_ids.cuda()\n",
    "        news_vec = model.news_encoder(input_ids)\n",
    "        news_vec = news_vec.to(torch.device(\"cpu\")).detach().numpy()\n",
    "        news_scoring.extend(news_vec)\n",
    "\n",
    "news_scoring = np.array(news_scoring)\n",
    "\n",
    "# doc_sim = 0\n",
    "# for _ in tqdm(range(1000000)):\n",
    "#     i = random.randrange(1, len(news_scoring))\n",
    "#     j = random.randrange(1, len(news_scoring))\n",
    "#     if i != j:\n",
    "#         doc_sim += np.dot(news_scoring[i], news_scoring[j]) / (np.linalg.norm(news_scoring[i]) * np.linalg.norm(news_scoring[j]))\n",
    "\n",
    "data_file_path = os.path.join(test_data_dir, f'behaviors.tsv')\n",
    "\n",
    "def collate_fn(tuple_list):\n",
    "    log_vecs = torch.FloatTensor([x[0] for x in tuple_list])\n",
    "    log_mask = torch.FloatTensor([x[1] for x in tuple_list])\n",
    "    news_vecs = [x[2] for x in tuple_list]\n",
    "    labels = [x[3] for x in tuple_list]\n",
    "    return (log_vecs, log_mask, news_vecs, labels)\n",
    "\n",
    "dataset = DatasetTest(data_file_path, news_index, news_scoring)\n",
    "dataloader = DataLoader(dataset, batch_size=128, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b400ec26-00de-4e8d-8066-9825b3528609",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUC = []\n",
    "MRR = []\n",
    "nDCG5 = []\n",
    "nDCG10 = []\n",
    "\n",
    "def print_metrics(cnt, x):\n",
    "    print(cnt, x)\n",
    "\n",
    "def get_mean(arr):\n",
    "    return [np.array(i).mean() for i in arr]\n",
    "\n",
    "def get_sum(arr):\n",
    "    return [np.array(i).sum() for i in arr]\n",
    "\n",
    "\n",
    "def dcg_score(y_true, y_score, k=10):\n",
    "    order = np.argsort(y_score)[::-1]\n",
    "    y_true = np.take(y_true, order[:k])\n",
    "    gains = 2**y_true - 1\n",
    "    discounts = np.log2(np.arange(len(y_true)) + 2)\n",
    "    return np.sum(gains / discounts)\n",
    "\n",
    "\n",
    "def ndcg_score(y_true, y_score, k=10):\n",
    "    best = dcg_score(y_true, y_true, k)\n",
    "    actual = dcg_score(y_true, y_score, k)\n",
    "    return actual / best\n",
    "\n",
    "\n",
    "def mrr_score(y_true, y_score):\n",
    "    order = np.argsort(y_score)[::-1]\n",
    "    y_true = np.take(y_true, order)\n",
    "    rr_score = y_true / (np.arange(len(y_true)) + 1)\n",
    "    return np.sum(rr_score) / np.sum(y_true)\n",
    "\n",
    "\n",
    "def ctr_score(y_true, y_score, k=1):\n",
    "    order = np.argsort(y_score)[::-1]\n",
    "    y_true = np.take(y_true, order[:k])\n",
    "    return np.mean(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8cc4a5b5-603b-4a2f-8b85-bc1aca07ded9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [0.6583311161846661, 0.28823356044382076, 0.30162660835532784, 0.3770027802180992]\n",
      "100 [0.663075304847043, 0.3198690653649036, 0.3531824126277258, 0.41437885925414053]\n",
      "200 [0.6612801711189159, 0.32131603397821135, 0.3540195760368032, 0.4159903165664601]\n",
      "300 [0.6603789272031168, 0.31893979938771344, 0.3512724441808343, 0.4140787979606762]\n",
      "400 [0.6599928270580613, 0.31718024348412643, 0.34940130085014703, 0.41200881140997625]\n",
      "500 [0.6593289509983885, 0.31680712138703115, 0.3490334193262708, 0.4119601314507803]\n",
      "571 [0.6591725605952566, 0.31704146764496266, 0.34940791521193026, 0.41236519077303857]\n"
     ]
    }
   ],
   "source": [
    "for cnt, (log_vecs, log_mask, news_vecs, labels) in enumerate(dataloader):\n",
    "    log_vecs = log_vecs.cuda()\n",
    "    log_mask = log_mask.cuda()\n",
    "\n",
    "    user_vecs = model.user_encoder(log_vecs, log_mask).to(torch.device(\"cpu\")).detach().numpy()\n",
    "\n",
    "    for user_vec, news_vec, label in zip(user_vecs, news_vecs, labels):\n",
    "        if label.mean() == 0 or label.mean() == 1:\n",
    "            continue\n",
    "\n",
    "        score = np.dot(news_vec, user_vec)\n",
    "        auc = roc_auc_score(label, score)\n",
    "        mrr = mrr_score(label, score)\n",
    "        ndcg5 = ndcg_score(label, score, k=5)\n",
    "        ndcg10 = ndcg_score(label, score, k=10)\n",
    "\n",
    "        AUC.append(auc)\n",
    "        MRR.append(mrr)\n",
    "        nDCG5.append(ndcg5)\n",
    "        nDCG10.append(ndcg10)\n",
    "\n",
    "    if cnt % 100 == 0:\n",
    "        print_metrics(cnt, get_mean([AUC, MRR, nDCG5, nDCG10]))\n",
    "\n",
    "print_metrics(cnt, get_mean([AUC, MRR, nDCG5, nDCG10]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
