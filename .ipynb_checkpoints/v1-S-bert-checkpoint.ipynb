{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a981dcde-34cf-453d-99d6-199397a96e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import random\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8efb0018-fc1c-4fc7-b190-56f26fe0bc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import IterableDataset, Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d64364a-4833-4a00-91f0-645c4dfce442",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample(all_elements, num_sample):\n",
    "    if num_sample > len(all_elements):\n",
    "        return random.sample(all_elements * (num_sample // len(all_elements) + 1), num_sample)\n",
    "    else:\n",
    "        return random.sample(all_elements, num_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d9b1aab-6372-4ddf-9b95-5d8d4f9fc719",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dir = './data/MINDsmall_train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff72edd8-ab14-4e12-ad04-364588b54211",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_training_data(seed = 1009, npratio = 4):\n",
    "    random.seed(seed)\n",
    "    behaviors = []\n",
    "    \n",
    "    behavior_file_path = os.path.join(train_data_dir, 'behaviors.tsv')\n",
    "    with open(behavior_file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in tqdm(f):\n",
    "            iid, uid, time, history, imp = line.strip().split('\\t')\n",
    "            his = history.split(' ')\n",
    "            # print(his, len(his))\n",
    "            if his[0] == '':\n",
    "                continue\n",
    "            impressions = [x.split('-') for x in imp.split(' ')]\n",
    "            pos, neg = [], []\n",
    "            for news_ID, label in impressions:\n",
    "                if label == '0':\n",
    "                    neg.append(news_ID)\n",
    "                elif label == '1':\n",
    "                    pos.append(news_ID)\n",
    "            if len(pos) == 0 or len(neg) == 0:\n",
    "                continue\n",
    "            for pos_id in pos:\n",
    "                neg_candidate = get_sample(neg, npratio)\n",
    "                neg_str = ' '.join(neg_candidate)\n",
    "                new_line = '\\t'.join([iid, uid, time, history, pos_id, neg_str]) + '\\n'\n",
    "                behaviors.append(new_line)\n",
    "\n",
    "    random.shuffle(behaviors)\n",
    "    processed_file_path = os.path.join(train_data_dir, f'behaviors_np{npratio}.tsv')\n",
    "    with open(processed_file_path, 'w') as f:\n",
    "        f.writelines(behaviors)\n",
    "    return len(behaviors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ac3f745-0923-4bcc-921d-fe0cc6c7cc7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "156965it [00:03, 50302.59it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "231530"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepare_training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e8e49f0-8cb4-4dee-bf45-d03e16779aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlap_info(set1, set2):\n",
    "    overlap_keys = set(set1.keys()) & set(set2.keys())\n",
    "    print(f\"Number of overlapping keys between news2int and news3int: {len(overlap_keys)}, lenset: {len(set1), len(set2)}\")\n",
    "    print(\"****set1****\")\n",
    "    for key in set1.keys():\n",
    "        if key not in overlap_keys:\n",
    "            print(key)\n",
    "    print(\"****set2****\")\n",
    "    for key in set2.keys():\n",
    "        if key not in overlap_keys:\n",
    "            print(key)\n",
    "\n",
    "\n",
    "def get_indexCate(dictCat, key):\n",
    "    if key in dictCat:\n",
    "        return dictCat[key]\n",
    "    print(len(dictCat), key)\n",
    "    return 0\n",
    "\n",
    "def update_dict(dict, key, value=None):\n",
    "    if key not in dict:\n",
    "        if value is None:\n",
    "            dict[key] = len(dict) + 1\n",
    "        else:\n",
    "            dict[key] = value\n",
    "            \n",
    "def read_news(news_path, tt_mat = None, mode = 'train'):\n",
    "    news = {}\n",
    "    newsVT = {}\n",
    "    news_index = {}\n",
    "    tt_emb = {}\n",
    "    counter = 0\n",
    "    with open(news_path, 'r', encoding=\"utf8\") as ifile:\n",
    "        news_collection = ifile.readlines()\n",
    "        for line in tqdm(news_collection):\n",
    "            newsid, category, subcategory, title, abstract, _, _, _ = line.strip().split(\"\\t\")\n",
    "            if newsid in news:\n",
    "                continue\n",
    "            emtt = tt_mat[counter]\n",
    "            counter += 1\n",
    "            update_dict(news, newsid, [emtt, category, subcategory])\n",
    "            update_dict(news_index, newsid)\n",
    "            if mode == 'train':\n",
    "                update_dict(category_dict, category)\n",
    "                update_dict(subcategory_dict, subcategory)\n",
    "            ft = np.concatenate((emtt, [get_indexCate(category_dict, category), get_indexCate(subcategory_dict, subcategory)]))\n",
    "            update_dict(newsVT, newsid, ft)\n",
    "\n",
    "    assert  tt_mat.shape[0] == len(news)\n",
    "    return news, news_index, newsVT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "c986b191-f2c8-4d08-82e5-27119ea48fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 51282/51282 [00:01<00:00, 50312.30it/s]\n"
     ]
    }
   ],
   "source": [
    "title_encoded = np.load('./data/tt_mat_train.npy')\n",
    "abs_encoded = np.load('./data/at_mat_train.npy')\n",
    "tt = np.hstack([title_encoded, abs_encoded])\n",
    "category_dict = {}\n",
    "subcategory_dict = {}\n",
    "news, news_index, newsVT = read_news(os.path.join(train_data_dir, 'news.tsv'), tt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce7390a-6f0d-4565-b547-7fe273b3354b",
   "metadata": {},
   "source": [
    "### Prepare train Dataset\n",
    "Trim history of users which longer than 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d25920b2-9b51-4cc4-bb3a-e544490c3ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def listN2feat(listN, features):\n",
    "    listF = []\n",
    "    for newsID in listN:\n",
    "        listF.append(features[newsID])\n",
    "    return np.asarray(listF)\n",
    "def pad_to_fix_len(x, fix_length, padding_front=True, padding_value=0):\n",
    "    if padding_front:\n",
    "        pad_x = [padding_value] * (fix_length - len(x)) + x[-fix_length:]\n",
    "        mask = [0] * (fix_length - len(x)) + [1] * min(fix_length, len(x))\n",
    "    else:\n",
    "        pad_x = x[-fix_length:] + [padding_value] * (fix_length - len(x))\n",
    "        mask = [1] * min(fix_length, len(x)) + [0] * (fix_length - len(x))\n",
    "    return pad_x, np.array(mask, dtype='float32')\n",
    "\n",
    "def pad_matrix(matrix, pad_length):\n",
    "  \"\"\"Pads a numpy matrix with zeros to a specific shape.\n",
    "\n",
    "  Args:\n",
    "    matrix: The numpy matrix to pad.\n",
    "\n",
    "  Returns:\n",
    "    A new numpy matrix with zeros padded to shape (x,y).\n",
    "  \"\"\"\n",
    "  padded_matrix = np.zeros((pad_length, matrix.shape[1]))\n",
    "  padded_matrix[-matrix.shape[0]:, -matrix.shape[1]:] = matrix\n",
    "  return padded_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2eecd1c-3e33-4982-a6cf-ca452c594657",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetTrain(Dataset):\n",
    "    def __init__(self, path, news, fullVT):\n",
    "        self.data = path\n",
    "        self.news = news\n",
    "        self.user_log_length = 100\n",
    "        self.npratio = 4\n",
    "        self.behaviors = []\n",
    "        self.newsVT = fullVT\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            for line in tqdm(f):\n",
    "                iid, uid, time, history, pos, neg = line.strip().split('\\t')\n",
    "                negs = [x.split('-')[0] for x in neg.split(' ')]\n",
    "                histories = [x.split('-')[0] for x in history.split(' ')]\n",
    "                if len(histories) > 100:\n",
    "                    histories = histories[-100:]\n",
    "                self.behaviors.append([uid, histories, pos, negs])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        uid, histories, pos, negs = self.behaviors[idx]\n",
    "        user_feature = pad_matrix(listN2feat(histories, self.newsVT), self.user_log_length)\n",
    "        _, log_mask = pad_to_fix_len(histories, self.user_log_length)\n",
    "        label = random.randint(0, self.npratio)\n",
    "        sample_news = negs[:label]\n",
    "        sample_news.append(pos)\n",
    "        sample_news.extend(negs[label:])\n",
    "        new_ft = listN2feat(sample_news, self.newsVT)\n",
    "        \n",
    "        return user_feature, log_mask, new_ft, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.behaviors)\n",
    "\n",
    "class DatasetTest(Dataset):\n",
    "    def __init__(self, path, scoring):\n",
    "        self.data = path\n",
    "        self.user_log_length = 100\n",
    "        self.behaviors = []\n",
    "        self.scoring = scoring\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            for line in tqdm(f):\n",
    "                iid, uid, time, history, imp = line.strip().split('\\t')\n",
    "                histories = [x.split('-')[0] for x in history.split(' ')]\n",
    "                if histories[0] == '':\n",
    "                    continue\n",
    "                candidates = [x.split('-')[0] for x in imp.split(' ')]\n",
    "                labels = np.array([int(i.split('-')[1]) for i in imp.split()])\n",
    "                if len(histories) > 100:\n",
    "                    histories = histories[-100:]\n",
    "                self.behaviors.append([uid, histories, candidates, labels])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        uid, histories, candidates, labels = self.behaviors[idx]\n",
    "        \n",
    "        user_feature = pad_matrix(listN2feat(histories, self.scoring), self.user_log_length)\n",
    "        _, log_mask = pad_to_fix_len(histories, self.user_log_length)\n",
    "        \n",
    "        new_ft = listN2feat(candidates, self.scoring)\n",
    "        \n",
    "        return user_feature, log_mask, new_ft, labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.behaviors)\n",
    "\n",
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.listK = list(data.keys())\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.listK[idx], self.data[self.listK[idx]]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.listK)\n",
    "\n",
    "\n",
    "\n",
    "def collate_fn(tuple_list):\n",
    "    log_vecs = torch.FloatTensor([x[0] for x in tuple_list])\n",
    "    log_mask = torch.FloatTensor([x[1] for x in tuple_list])\n",
    "    news_vecs = [x[2] for x in tuple_list]\n",
    "    labels = [x[3] for x in tuple_list]\n",
    "    return (log_vecs, log_mask, news_vecs, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "25c6ff7f-4a0b-4a43-a2eb-3509e518afd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "231530it [00:03, 71091.92it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = DatasetTrain(train_data_dir + '/behaviors_np4.tsv', news, newsVT)\n",
    "dataloader = DataLoader(dataset, batch_size=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "d7d26cfe-093e-4ad4-bd1d-45d2d0a84342",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionPooling(nn.Module):\n",
    "    def __init__(self, emb_size, hidden_size):\n",
    "        super(AttentionPooling, self).__init__()\n",
    "        self.att_fc1 = nn.Linear(emb_size, hidden_size)\n",
    "        self.att_fc2 = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x, attn_mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: batch_size, candidate_size, emb_dim\n",
    "            attn_mask: batch_size, candidate_size\n",
    "        Returns:\n",
    "            (shape) batch_size, emb_dim\n",
    "        \"\"\"\n",
    "        e = self.att_fc1(x)\n",
    "        e = nn.Tanh()(e)\n",
    "        alpha = self.att_fc2(e)\n",
    "        alpha = torch.exp(alpha)\n",
    "\n",
    "        if attn_mask is not None:\n",
    "            alpha = alpha * attn_mask.unsqueeze(2)\n",
    "\n",
    "        alpha = alpha / (torch.sum(alpha, dim=1, keepdim=True) + 1e-8)\n",
    "        x = torch.bmm(x.permute(0, 2, 1), alpha).squeeze(dim=-1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, d_k):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "        self.d_k = d_k\n",
    "\n",
    "    def forward(self, Q, K, V, attn_mask=None):\n",
    "        '''\n",
    "            Q: batch_size, n_head, candidate_num, d_k\n",
    "            K: batch_size, n_head, candidate_num, d_k\n",
    "            V: batch_size, n_head, candidate_num, d_v\n",
    "            attn_mask: batch_size, n_head, candidate_num\n",
    "            Return: batch_size, n_head, candidate_num, d_v\n",
    "        '''\n",
    "        scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(self.d_k)\n",
    "        scores = torch.exp(scores)\n",
    "\n",
    "        if attn_mask is not None:\n",
    "            scores = scores * attn_mask.unsqueeze(dim=-2)\n",
    "\n",
    "        attn = scores / (torch.sum(scores, dim=-1, keepdim=True) + 1e-8)\n",
    "        context = torch.matmul(attn, V)\n",
    "        return context\n",
    "\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_k, d_v):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "\n",
    "        self.W_Q = nn.Linear(d_model, d_k * n_heads)\n",
    "        self.W_K = nn.Linear(d_model, d_k * n_heads)\n",
    "        self.W_V = nn.Linear(d_model, d_v * n_heads)\n",
    "\n",
    "        self.scaled_dot_product_attn = ScaledDotProductAttention(self.d_k)\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight, gain=1)\n",
    "\n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        '''\n",
    "            Q: batch_size, candidate_num, d_model\n",
    "            K: batch_size, candidate_num, d_model\n",
    "            V: batch_size, candidate_num, d_model\n",
    "            mask: batch_size, candidate_num\n",
    "        '''\n",
    "        batch_size = Q.shape[0]\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(dim=1).expand(-1, self.n_heads, -1)\n",
    "\n",
    "        q_s = self.W_Q(Q).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        k_s = self.W_K(K).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        v_s = self.W_V(V).view(batch_size, -1, self.n_heads, self.d_v).transpose(1, 2)\n",
    "\n",
    "        context = self.scaled_dot_product_attn(q_s, k_s, v_s, mask)\n",
    "        output = context.transpose(1, 2).contiguous().view(batch_size, -1, self.n_heads * self.d_v)\n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def print_metrics(cnt, x):\n",
    "    print(cnt, x)\n",
    "\n",
    "def get_mean(arr):\n",
    "    return [np.array(i).mean() for i in arr]\n",
    "\n",
    "def get_sum(arr):\n",
    "    return [np.array(i).sum() for i in arr]\n",
    "\n",
    "\n",
    "def dcg_score(y_true, y_score, k=10):\n",
    "    order = np.argsort(y_score)[::-1]\n",
    "    y_true = np.take(y_true, order[:k])\n",
    "    gains = 2**y_true - 1\n",
    "    discounts = np.log2(np.arange(len(y_true)) + 2)\n",
    "    return np.sum(gains / discounts)\n",
    "\n",
    "\n",
    "def ndcg_score(y_true, y_score, k=10):\n",
    "    best = dcg_score(y_true, y_true, k)\n",
    "    actual = dcg_score(y_true, y_score, k)\n",
    "    return actual / best\n",
    "\n",
    "\n",
    "def mrr_score(y_true, y_score):\n",
    "    order = np.argsort(y_score)[::-1]\n",
    "    y_true = np.take(y_true, order)\n",
    "    rr_score = y_true / (np.arange(len(y_true)) + 1)\n",
    "    return np.sum(rr_score) / np.sum(y_true)\n",
    "\n",
    "\n",
    "def ctr_score(y_true, y_score, k=1):\n",
    "    order = np.argsort(y_score)[::-1]\n",
    "    y_true = np.take(y_true, order[:k])\n",
    "    return np.mean(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b1f1de31-d838-4caa-a481-eed704c50759",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsEncoder(nn.Module):\n",
    "    def __init__(self, num_category, num_subcategory):\n",
    "        super(NewsEncoder, self).__init__()\n",
    "        self.drop_rate = 0.2\n",
    "        category_emb_dim = 100\n",
    "        news_dim = 768\n",
    "        news_query_vector_dim = 200\n",
    "        self.ttEmb = nn.Linear(news_dim, news_dim)\n",
    "        self.category_emb = nn.Embedding(num_category + 1, category_emb_dim, padding_idx=0)\n",
    "        self.category_dense = nn.Linear(category_emb_dim, news_dim)\n",
    "        self.subcategory_emb = nn.Embedding(num_subcategory + 1, category_emb_dim, padding_idx=0)\n",
    "        self.subcategory_dense = nn.Linear(category_emb_dim, news_dim)\n",
    "        self.final_attn = AttentionPooling(news_dim, news_query_vector_dim)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        '''\n",
    "            x: batch_size, word_num\n",
    "            mask: batch_size, word_num\n",
    "        '''\n",
    "        title = x.reshape(-1, 770)[:,:-2].float()\n",
    "        category = x[..., -2].reshape(-1).long()\n",
    "        subcat = x[..., -1].reshape(-1).long()\n",
    "    \n",
    "        title = self.ttEmb(title)\n",
    "        all_vecs = [title]\n",
    "\n",
    "        category_vecs = self.category_dense(self.category_emb(category))\n",
    "        all_vecs.append(category_vecs)\n",
    "        subcategory_vecs = self.subcategory_dense(self.subcategory_emb(subcat))\n",
    "        all_vecs.append(subcategory_vecs)\n",
    "        \n",
    "        # all_vecs = torch.cat(all_vecs, dim=1)\n",
    "        all_vecs = torch.stack(all_vecs, dim=1)\n",
    "        news_vecs = self.final_attn(all_vecs)\n",
    "        return news_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "5af21979-d05f-42b3-8235-83c1b43d272a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class UserEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UserEncoder, self).__init__()\n",
    "        news_dim = 768\n",
    "        user_query_vector_dim = 200\n",
    "        self.user_log_length = 100\n",
    "        self.user_log_mask = False\n",
    "        self.attn = AttentionPooling(news_dim, user_query_vector_dim)\n",
    "        self.pad_doc = nn.Parameter(torch.empty(1, news_dim).uniform_(-1, 1)).type(torch.FloatTensor)\n",
    "\n",
    "    def forward(self, news_vecs, log_mask=None):\n",
    "        '''\n",
    "            news_vecs: batch_size, history_num, news_dim\n",
    "            log_mask: batch_size, history_num\n",
    "        '''\n",
    "        bz = news_vecs.shape[0]\n",
    "        if self.user_log_mask:\n",
    "            user_vec = self.attn(news_vecs, log_mask)\n",
    "        else:\n",
    "            padding_doc = self.pad_doc.unsqueeze(dim=0).expand(bz, self.user_log_length, -1)\n",
    "            news_vecs = news_vecs * log_mask.unsqueeze(dim=-1) + padding_doc * (1 - log_mask.unsqueeze(dim=-1))\n",
    "            user_vec = self.attn(news_vecs)\n",
    "        return user_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "455ee000-ecf0-4661-973c-bdf52246e33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NAML(torch.nn.Module):\n",
    "    def __init__(self, num_category, num_subcategory, **kwargs):\n",
    "        super(NAML, self).__init__()\n",
    "        self.news_encoder = NewsEncoder(num_category, num_subcategory)\n",
    "        self.user_encoder = UserEncoder()\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        self.npratio = 4\n",
    "        self.news_dim = 768\n",
    "        self.user_log_length = 100\n",
    "\n",
    "    def forward(self, history, history_mask, candidate, label):\n",
    "        '''\n",
    "            history: batch_size, history_length, num_word_title\n",
    "            history_mask: batch_size, history_length\n",
    "            candidate: batch_size, 1+K, num_word_title\n",
    "            label: batch_size, 1+K\n",
    "        '''\n",
    "        candidate_news_vecs = self.news_encoder(candidate).reshape(-1, 1 + self.npratio, self.news_dim)\n",
    "        # print(\"candidate: \")\n",
    "        history_news_vecs = self.news_encoder(history).reshape(-1, self.user_log_length, self.news_dim)\n",
    "        # print(\"history_news_vecs: \")\n",
    "        user_vec = self.user_encoder(history_news_vecs, history_mask)\n",
    "        score = torch.bmm(candidate_news_vecs, user_vec.unsqueeze(dim=-1)).squeeze(dim=-1)\n",
    "        loss = self.loss_fn(score, label)\n",
    "        # stop\n",
    "        return loss, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8a5cdc7e-773c-4dcf-8682-e0fc46d09806",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def acc(y_true, y_hat):\n",
    "    y_hat = torch.argmax(y_hat, dim=-1)\n",
    "    tot = y_true.shape[0]\n",
    "    hit = torch.sum(y_true == y_hat)\n",
    "    return hit.data.float() * 1.0 / tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "71fcf8f3-f1dc-45a0-b6a2-1dc4f56d6a1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NAML(\n",
       "  (news_encoder): NewsEncoder(\n",
       "    (ttEmb): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (category_emb): Embedding(18, 100, padding_idx=0)\n",
       "    (category_dense): Linear(in_features=100, out_features=768, bias=True)\n",
       "    (subcategory_emb): Embedding(265, 100, padding_idx=0)\n",
       "    (subcategory_dense): Linear(in_features=100, out_features=768, bias=True)\n",
       "    (final_attn): AttentionPooling(\n",
       "      (att_fc1): Linear(in_features=768, out_features=200, bias=True)\n",
       "      (att_fc2): Linear(in_features=200, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (user_encoder): UserEncoder(\n",
       "    (attn): AttentionPooling(\n",
       "      (att_fc1): Linear(in_features=768, out_features=200, bias=True)\n",
       "      (att_fc2): Linear(in_features=200, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (loss_fn): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train():\n",
    "model = NAML(len(category_dict), len(subcategory_dict))\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0003)\n",
    "model = model.cuda()\n",
    "torch.set_grad_enabled(True)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "3fe4dc0a-cbd4-4dba-adbd-f067fa3371e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1809it [01:51, 16.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2530.5469, device='cuda:0') tensor(781.0856, device='cuda:0')\n",
      "EPOCH: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1809it [01:50, 16.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2424.9500, device='cuda:0') tensor(832.4359, device='cuda:0')\n",
      "EPOCH: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1809it [01:50, 16.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2392.2183, device='cuda:0') tensor(851.2969, device='cuda:0')\n",
      "EPOCH: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1809it [01:50, 16.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2368.9851, device='cuda:0') tensor(864.1279, device='cuda:0')\n",
      "EPOCH: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1809it [01:50, 16.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2349.6792, device='cuda:0') tensor(874.0905, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for ep in range(5):\n",
    "    loss = 0.0\n",
    "    accuary = 0.0\n",
    "    print(\"EPOCH: \" + str(ep))\n",
    "    for cnt, (log_ids, log_mask, input_ids, targets) in tqdm(enumerate(dataloader)):\n",
    "        log_ids = log_ids.cuda()\n",
    "        log_mask = log_mask.cuda()\n",
    "        input_ids = input_ids.cuda()\n",
    "        targets = targets.cuda()\n",
    "\n",
    "        bz_loss, y_hat = model(log_ids, log_mask, input_ids, targets)\n",
    "        loss += bz_loss.data.float()\n",
    "        accuary += acc(targets, y_hat)\n",
    "        optimizer.zero_grad()\n",
    "        bz_loss.backward()\n",
    "        optimizer.step()\n",
    "        # stop\n",
    "    print(loss, accuary)\n",
    "\n",
    "    #     if rank == 0 and cnt != 0 and cnt % args.save_steps == 0:\n",
    "    #         ckpt_path = os.path.join(args.model_dir, f'epoch-{ep+1}-{cnt}.pt')\n",
    "    #         torch.save(\n",
    "    #             {\n",
    "    #                 'model_state_dict':\n",
    "    #                     {'.'.join(k.split('.')[1:]): v for k, v in model.state_dict().items()}\n",
    "    #                     if is_distributed else model.state_dict(),\n",
    "    #                 'category_dict': category_dict,\n",
    "    #                 'word_dict': word_dict,\n",
    "    #                 'subcategory_dict': subcategory_dict\n",
    "    #             }, ckpt_path)\n",
    "    #         logging.info(f\"Model saved to {ckpt_path}.\")\n",
    "\n",
    "    # logging.info('Training finish.')\n",
    "\n",
    "    # if rank == 0:\n",
    "    #     ckpt_path = os.path.join(args.model_dir, f'epoch-{ep+1}.pt')\n",
    "    #     torch.save(\n",
    "    #         {\n",
    "    #             'model_state_dict':\n",
    "    #                 {'.'.join(k.split('.')[1:]): v for k, v in model.state_dict().items()}\n",
    "    #                 if is_distributed else model.state_dict(),\n",
    "    #             'category_dict': category_dict,\n",
    "    #             'subcategory_dict': subcategory_dict,\n",
    "    #             'word_dict': word_dict,\n",
    "    #         }, ckpt_path)\n",
    "    #     logging.info(f\"Model saved to {ckpt_path}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea8a0dc-670d-487e-9119-2e376c46fc86",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "07a1e9b1-356e-453c-97ae-160e10c9a982",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_dir = './data/MINDsmall_dev'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "462e0c7c-3857-446c-b4f6-9e53252306a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|████████████████████████████████████████████████▉                       | 28820/42416 [00:00<00:00, 133625.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "264 lifestyleanimals\n",
      "264 shop-computers-electronics\n",
      "264 lifestyletravel\n",
      "17 games\n",
      "264 games-news\n",
      "264 newsvideos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 42416/42416 [00:00<00:00, 130458.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "264 newstechnology\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.cuda()\n",
    "model.eval()\n",
    "torch.set_grad_enabled(False)\n",
    "title_encoded = np.load('./data/tt_mat_test.npy')\n",
    "abs_encoded = np.load('./data/at_mat_test.npy')\n",
    "tt = np.hstack([title_encoded, abs_encoded])\n",
    "news, news_index, newsVT_test = read_news(os.path.join(test_data_dir, 'news.tsv'), tt, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "c4814240-f5db-4927-9bb9-2d616cc36bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_dataset = NewsDataset(newsVT_test)\n",
    "news_dataloader = DataLoader(news_dataset, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "aff50686-a512-4088-98c7-57945f68a6ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 332/332 [00:00<00:00, 361.05it/s]\n"
     ]
    }
   ],
   "source": [
    "news_scoring = {}\n",
    "with torch.no_grad():\n",
    "    for k, input_ids in tqdm(news_dataloader):\n",
    "        input_ids = input_ids.cuda()\n",
    "        news_vec = model.news_encoder(input_ids)\n",
    "        news_vec = news_vec.to(torch.device(\"cpu\")).detach().numpy()\n",
    "        for idx, eachK in enumerate(k):\n",
    "            news_scoring[eachK] = news_vec[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "a9045549-38ad-41d0-82ae-cd4c335bddd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "73152it [00:02, 26983.24it/s]\n"
     ]
    }
   ],
   "source": [
    "data_file_path = os.path.join(test_data_dir, f'behaviors.tsv')\n",
    "\n",
    "datasetTest = DatasetTest(data_file_path, news_scoring)\n",
    "dataloaderTest = DataLoader(datasetTest, batch_size=128, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "8cc4a5b5-603b-4a2f-8b85-bc1aca07ded9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [0.6587261480798572, 0.3255909975479099, 0.3560354229500778, 0.4207368253952161]\n",
      "100 [0.6515517596013197, 0.3162897624862829, 0.3464775701666316, 0.4085870939247561]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[105], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m log_vecs \u001b[38;5;241m=\u001b[39m log_vecs\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m      7\u001b[0m log_mask \u001b[38;5;241m=\u001b[39m log_mask\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m----> 9\u001b[0m user_vecs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muser_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlog_vecs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_mask\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m user_vec, news_vec, label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(user_vecs, news_vecs, labels):\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m label\u001b[38;5;241m.\u001b[39mmean() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m label\u001b[38;5;241m.\u001b[39mmean() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "AUC = []\n",
    "MRR = []\n",
    "nDCG5 = []\n",
    "nDCG10 = []\n",
    "for cnt, (log_vecs, log_mask, news_vecs, labels) in enumerate(dataloaderTest):\n",
    "    log_vecs = log_vecs.cuda()\n",
    "    log_mask = log_mask.cuda()\n",
    "\n",
    "    user_vecs = model.user_encoder(log_vecs, log_mask).to(torch.device(\"cpu\")).detach().numpy()\n",
    "\n",
    "    for user_vec, news_vec, label in zip(user_vecs, news_vecs, labels):\n",
    "        if label.mean() == 0 or label.mean() == 1:\n",
    "            continue\n",
    "\n",
    "        score = np.dot(news_vec, user_vec)\n",
    "\n",
    "        auc = roc_auc_score(label, score)\n",
    "        mrr = mrr_score(label, score)\n",
    "        ndcg5 = ndcg_score(label, score, k=5)\n",
    "        ndcg10 = ndcg_score(label, score, k=10)\n",
    "\n",
    "        AUC.append(auc)\n",
    "        MRR.append(mrr)\n",
    "        nDCG5.append(ndcg5)\n",
    "        nDCG10.append(ndcg10)\n",
    "\n",
    "    if cnt % 100 == 0:\n",
    "        print_metrics(cnt, get_mean([AUC, MRR, nDCG5, nDCG10]))\n",
    "\n",
    "print_metrics(cnt, get_mean([AUC, MRR, nDCG5, nDCG10]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
