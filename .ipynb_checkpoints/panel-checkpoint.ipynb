{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9ab791d-14da-4b4d-b707-685d8e07a9cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\dangkh\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from src.config import TrainConfig \n",
    "from src.ultis import *\n",
    "from src.data_helper import prepare_preprocessed_data\n",
    "from src.data_load import *\n",
    "from src.metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00f99f0e-411a-4e2a-a607-e1eced00819e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e03912b9-d773-4763-90ca-b435671b6079",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data, Batch\n",
    "from torch_geometric.utils import subgraph\n",
    "\n",
    "from torch.utils.data import IterableDataset\n",
    "from torch_geometric.loader import DataLoader as GraphDataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "129e3122-9b9f-43c3-87f8-1364326cbc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "device: torch.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f84c11bf-8379-4420-8913-c4636faaddc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = TrainConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4228444-36ca-48bd-831b-673e449dc985",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_random_seed(cfg.random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ddb2613-8bfb-4ab3-a65a-bff0bab3933a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-13 15:26:28,116 INFO Start\n",
      "2024-10-13 15:26:28,117 INFO Prepare the dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] All preprocessed files exist.\n",
      "[val] All preprocessed files exist.\n",
      "[train] All graphs exist !\n",
      "[val] All graphs exist !\n",
      "[train] Start to process neighbors list\n",
      "[train] All news Neighbor dict exist !\n",
      "[val] Start to process neighbors list\n",
      "[val] All news Neighbor dict exist !\n",
      "[train] Entity graph exists!\n",
      "[val] Entity graph exists!\n",
      "[train] Start to process neighbors list\n",
      "[train] All entity Neighbor dict exist !\n",
      "[val] Start to process neighbors list\n",
      "[val] All entity Neighbor dict exist !\n"
     ]
    }
   ],
   "source": [
    "logging.info(\"Start\")\n",
    "\"\"\"\n",
    "0. Definite Parameters & Functions\n",
    "\n",
    "\"\"\"\n",
    "logging.info(\"Prepare the dataset\")\n",
    "prepare_preprocessed_data(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d675ac1-49b8-40d9-bb0f-72eb155356b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mode='train'\n",
    "data_dir = {\"train\": cfg.data_dir + '_train', \"val\": cfg.data_dir + '_val', \"test\": cfg.data_dir}\n",
    "\n",
    "# ------------- load news.tsv-------------\n",
    "news_index = pickle.load(open(Path(data_dir[mode]) / \"news_dict.bin\", \"rb\"))\n",
    "\n",
    "news_input = pickle.load(open(Path(data_dir[mode]) / \"nltk_token_news.bin\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be0621d9-4d85-45e6-b060-60f0289ce1de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] News Graph Info: Data(x=[51283, 22], edge_index=[2, 1171966], edge_attr=[1171966], num_nodes=51283)\n"
     ]
    }
   ],
   "source": [
    "target_file = Path(data_dir[mode]) / f\"behaviors_np{cfg.npratio}_0.tsv\"\n",
    "news_graph = torch.load(Path(data_dir[mode]) / \"nltk_news_graph.pt\")\n",
    "\n",
    "if cfg.directed is False:\n",
    "    news_graph.edge_index, news_graph.edge_attr = to_undirected(news_graph.edge_index, news_graph.edge_attr)\n",
    "print(f\"[{mode}] News Graph Info: {news_graph}\")\n",
    "\n",
    "news_neighbors_dict = pickle.load(open(Path(data_dir[mode]) / \"news_neighbor_dict.bin\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cd74e17d-6309-4a94-a130-9f5108f4be3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset_PANEL_1(IterableDataset):\n",
    "    def __init__(self, filename, news_index, news_combined, cfg, neighbor_dict, news_graph):\n",
    "        super(Dataset_PANEL_1).__init__()\n",
    "        self.filename = filename\n",
    "        self.news_index = news_index\n",
    "        self.news_combined = news_combined\n",
    "        self.user_log_length = cfg.his_size\n",
    "        self.npratio = cfg.npratio\n",
    "        self.cfg = cfg\n",
    "        self.neighbor_dict = neighbor_dict\n",
    "        self.news_graph = news_graph\n",
    "\n",
    "    def trans_to_nindex(self, nids):\n",
    "        return [self.news_index[i] if i in self.news_index else 0 for i in nids]\n",
    "\n",
    "    def pad_to_fix_len(self, x, fix_length, padding_front=True, padding_value=0):\n",
    "        if padding_front:\n",
    "            pad_x = [padding_value] * (fix_length - len(x)) + x[-fix_length:]\n",
    "            mask = [0] * (fix_length - len(x)) + [1] * min(fix_length, len(x))\n",
    "        else:\n",
    "            pad_x = x[-fix_length:] + [padding_value] * (fix_length - len(x))\n",
    "            mask = [1] * min(fix_length, len(x)) + [0] * (fix_length - len(x))\n",
    "        return pad_x, np.array(mask, dtype='float32')\n",
    "\n",
    "    def build_k_hop(self, click_doc):\n",
    "        click_idx = [x for x in click_doc]\n",
    "        source_idx = [x for x in click_doc]\n",
    "        for _ in range(self.cfg.k_hops) :\n",
    "            current_hop_idx = []\n",
    "            for news_idx in source_idx:\n",
    "                current_hop_idx.extend(self.neighbor_dict[news_idx][:self.cfg.num_neighbors])\n",
    "            source_idx = current_hop_idx\n",
    "            click_idx.extend(current_hop_idx)\n",
    "        return list(set(click_idx))\n",
    "        \n",
    "\n",
    "    def line_mapper(self, line):\n",
    "        line = line.strip().split('\\t')\n",
    "        click_docs = line[3].split()\n",
    "        sess_pos = line[4].split()\n",
    "        sess_neg = line[5].split()\n",
    "        click_docs = self.trans_to_nindex(click_docs)\n",
    "\n",
    "        # build sub-graph\n",
    "        k_hops_click = self.build_k_hop(click_docs)\n",
    "        \n",
    "        # subemb = self.news_graph.x[k_hops_click]\n",
    "        # sub_edge_index, sub_edge_attr = subgraph(k_hops_click, self.news_graph.edge_index, self.news_graph.edge_attr, \\\n",
    "        #                                          relabel_nodes=True, num_nodes=self.news_graph.num_nodes)\n",
    "        # sub_news_graph = Data(x=subemb, edge_index=sub_edge_index, edge_attr=sub_edge_attr)\n",
    "\n",
    "        \n",
    "        click_docs, log_mask = self.pad_to_fix_len(click_docs, self.user_log_length)\n",
    "        user_feature = self.news_combined[click_docs]\n",
    "\n",
    "        pos = self.trans_to_nindex(sess_pos)\n",
    "        neg = self.trans_to_nindex(sess_neg)\n",
    "\n",
    "        label = random.randint(0, self.npratio)\n",
    "        sample_news = neg[:label] + pos + neg[label:]\n",
    "        news_feature = self.news_combined[sample_news]\n",
    "        return \"\", torch.from_numpy(user_feature), torch.from_numpy(log_mask), \\\n",
    "        torch.from_numpy(news_feature), torch.tensor(label)\n",
    "\n",
    "    def __iter__(self):\n",
    "        file_iter = open(self.filename)\n",
    "        return map(self.line_mapper, file_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4902e54f-cfe0-4e46-a1d2-9f4fb94a3adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset_PANEL_1(\n",
    "                filename=target_file,\n",
    "                news_index=news_index,\n",
    "                news_combined=news_input,\n",
    "                cfg=cfg,\n",
    "                neighbor_dict=news_neighbors_dict,\n",
    "                news_graph=news_graph\n",
    ")\n",
    "dataloader = GraphDataLoader(dataset, batch_size=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d9505e17-0339-4677-9bcf-3b1fddee7b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterator = iter(dataloader)\n",
    "# data_batch = next(iterator)\n",
    "# sub_news_graph, user_feature, log_mask, news_feature, label = data_batch\n",
    "# user_feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "84502714-78b3-4e44-a5df-530935c54695",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionPooling(nn.Module):\n",
    "    def __init__(self, emb_size, hidden_size):\n",
    "        super(AttentionPooling, self).__init__()\n",
    "        self.att_fc1 = nn.Linear(emb_size, hidden_size)\n",
    "        self.att_fc2 = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x, attn_mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: batch_size, candidate_size, emb_dim\n",
    "            attn_mask: batch_size, candidate_size\n",
    "        Returns:\n",
    "            (shape) batch_size, emb_dim\n",
    "        \"\"\"\n",
    "        e = self.att_fc1(x)\n",
    "        e = nn.Tanh()(e)\n",
    "        alpha = self.att_fc2(e)\n",
    "        alpha = torch.exp(alpha)\n",
    "\n",
    "        if attn_mask is not None:\n",
    "            alpha = alpha * attn_mask.unsqueeze(2)\n",
    "\n",
    "        alpha = alpha / (torch.sum(alpha, dim=1, keepdim=True) + 1e-8)\n",
    "        x = torch.bmm(x.permute(0, 2, 1), alpha).squeeze(dim=-1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, d_k):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "        self.d_k = d_k\n",
    "\n",
    "    def forward(self, Q, K, V, attn_mask=None):\n",
    "        '''\n",
    "            Q: batch_size, n_head, candidate_num, d_k\n",
    "            K: batch_size, n_head, candidate_num, d_k\n",
    "            V: batch_size, n_head, candidate_num, d_v\n",
    "            attn_mask: batch_size, n_head, candidate_num\n",
    "            Return: batch_size, n_head, candidate_num, d_v\n",
    "        '''\n",
    "        scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(self.d_k)\n",
    "        scores = torch.exp(scores)\n",
    "\n",
    "        if attn_mask is not None:\n",
    "            scores = scores * attn_mask.unsqueeze(dim=-2)\n",
    "\n",
    "        attn = scores / (torch.sum(scores, dim=-1, keepdim=True) + 1e-8)\n",
    "        context = torch.matmul(attn, V)\n",
    "        return context\n",
    "\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_k, d_v):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "\n",
    "        self.W_Q = nn.Linear(d_model, d_k * n_heads)\n",
    "        self.W_K = nn.Linear(d_model, d_k * n_heads)\n",
    "        self.W_V = nn.Linear(d_model, d_v * n_heads)\n",
    "\n",
    "        self.scaled_dot_product_attn = ScaledDotProductAttention(self.d_k)\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight, gain=1)\n",
    "\n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        '''\n",
    "            Q: batch_size, candidate_num, d_model\n",
    "            K: batch_size, candidate_num, d_model\n",
    "            V: batch_size, candidate_num, d_model\n",
    "            mask: batch_size, candidate_num\n",
    "        '''\n",
    "        batch_size = Q.shape[0]\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(dim=1).expand(-1, self.n_heads, -1)\n",
    "\n",
    "        q_s = self.W_Q(Q).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        k_s = self.W_K(K).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        v_s = self.W_V(V).view(batch_size, -1, self.n_heads, self.d_v).transpose(1, 2)\n",
    "\n",
    "        context = self.scaled_dot_product_attn(q_s, k_s, v_s, mask)\n",
    "        output = context.transpose(1, 2).contiguous().view(batch_size, -1, self.n_heads * self.d_v)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e4eb076-b86c-4d29-997e-59ab12de9091",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsEncoder(nn.Module):\n",
    "    def __init__(self, embedding_matrix, num_category, num_subcategory):\n",
    "        super(NewsEncoder, self).__init__()\n",
    "        self.embedding_matrix = embedding_matrix\n",
    "        self.drop_rate = 0.2\n",
    "        self.num_words_title = 20\n",
    "        self.use_category = True\n",
    "        self.use_subcategory = True\n",
    "        category_emb_dim = 100\n",
    "        news_dim = 400\n",
    "        news_query_vector_dim = 200\n",
    "        word_embedding_dim = 300\n",
    "        self.category_emb = nn.Embedding(num_category + 1, category_emb_dim, padding_idx=0)\n",
    "        self.category_dense = nn.Linear(category_emb_dim, news_dim)\n",
    "        self.subcategory_emb = nn.Embedding(num_subcategory + 1, category_emb_dim, padding_idx=0)\n",
    "        self.subcategory_dense = nn.Linear(category_emb_dim, news_dim)\n",
    "        self.final_attn = AttentionPooling(news_dim, news_query_vector_dim)\n",
    "        self.cnn = nn.Conv1d(\n",
    "            in_channels=word_embedding_dim,\n",
    "            out_channels=news_dim,\n",
    "            kernel_size=3,\n",
    "            padding=1\n",
    "        )\n",
    "        self.attn = AttentionPooling(news_dim, news_query_vector_dim)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        '''\n",
    "            x: batch_size, word_num\n",
    "            mask: batch_size, word_num\n",
    "        '''\n",
    "        title = torch.narrow(x, -1, 0, self.num_words_title).long()\n",
    "        word_vecs = F.dropout(self.embedding_matrix(title),\n",
    "                              p=self.drop_rate,\n",
    "                              training=self.training)\n",
    "        context_word_vecs = self.cnn(word_vecs.transpose(1, 2)).transpose(1, 2)\n",
    "        \n",
    "        title_vecs = self.attn(context_word_vecs, mask)\n",
    "        all_vecs = [title_vecs]\n",
    "\n",
    "        start = self.num_words_title\n",
    "        if self.use_category:\n",
    "            category = torch.narrow(x, -1, start, 1).squeeze(dim=-1).long()\n",
    "            category_vecs = self.category_dense(self.category_emb(category))\n",
    "            all_vecs.append(category_vecs)\n",
    "            start += 1\n",
    "        if self.use_subcategory:\n",
    "            subcategory = torch.narrow(x, -1, start, 1).squeeze(dim=-1).long()\n",
    "            subcategory_vecs = self.subcategory_dense(self.subcategory_emb(subcategory))\n",
    "            all_vecs.append(subcategory_vecs)\n",
    "\n",
    "        if len(all_vecs) == 1:\n",
    "            news_vecs = all_vecs[0]\n",
    "        else:\n",
    "            all_vecs = torch.stack(all_vecs, dim=1)\n",
    "            \n",
    "            news_vecs = self.final_attn(all_vecs)\n",
    "        return news_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f05fd2f-652d-4eaa-8da0-85f503ff1447",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UserEncoder, self).__init__()\n",
    "        news_dim = 400\n",
    "        user_query_vector_dim = 200\n",
    "        self.user_log_length = 50\n",
    "        self.user_log_mask = False\n",
    "        self.attn = AttentionPooling(news_dim, user_query_vector_dim)\n",
    "        self.pad_doc = nn.Parameter(torch.empty(1, news_dim).uniform_(-1, 1)).type(torch.FloatTensor)\n",
    "\n",
    "    def forward(self, news_vecs, log_mask=None):\n",
    "        '''\n",
    "            news_vecs: batch_size, history_num, news_dim\n",
    "            log_mask: batch_size, history_num\n",
    "        '''\n",
    "        bz = news_vecs.shape[0]\n",
    "        if self.user_log_mask:\n",
    "            user_vec = self.attn(news_vecs, log_mask)\n",
    "        else:\n",
    "            padding_doc = self.pad_doc.unsqueeze(dim=0).expand(bz, self.user_log_length, -1)\n",
    "            news_vecs = news_vecs * log_mask.unsqueeze(dim=-1) + padding_doc * (1 - log_mask.unsqueeze(dim=-1))\n",
    "            user_vec = self.attn(news_vecs)\n",
    "        return user_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ddaa5be1-8b93-41e0-a685-38c74b4ff8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NAML(torch.nn.Module):\n",
    "    def __init__(self, embedding_matrix, num_category, num_subcategory, **kwargs):\n",
    "        super(NAML, self).__init__()\n",
    "        pretrained_word_embedding = torch.from_numpy(embedding_matrix).float()\n",
    "        word_embedding = nn.Embedding.from_pretrained(pretrained_word_embedding,\n",
    "                                                      freeze=False,\n",
    "                                                      padding_idx=0)\n",
    "\n",
    "        self.news_encoder = NewsEncoder( word_embedding, num_category, num_subcategory)\n",
    "        self.user_encoder = UserEncoder()\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        self.npratio = 4\n",
    "        self.news_dim = 400\n",
    "        self.user_log_length = 50\n",
    "\n",
    "    def forward(self, history, history_mask, candidate, label):\n",
    "        '''\n",
    "            history: batch_size, history_length, num_word_title\n",
    "            history_mask: batch_size, history_length\n",
    "            candidate: batch_size, 1+K, num_word_title\n",
    "            label: batch_size, 1+K\n",
    "        '''\n",
    "        num_words = history.shape[-1]\n",
    "        candidate_news = candidate.reshape(-1, num_words)\n",
    "        candidate_news_vecs = self.news_encoder(candidate_news).reshape(-1, 1 + self.npratio, self.news_dim)\n",
    "        history_news = history.reshape(-1, num_words)\n",
    "        history_news_vecs = self.news_encoder(history_news).reshape(-1, self.user_log_length, self.news_dim)\n",
    "        user_vec = self.user_encoder(history_news_vecs, history_mask)\n",
    "        score = torch.bmm(candidate_news_vecs, user_vec.unsqueeze(dim=-1)).squeeze(dim=-1)\n",
    "        loss = self.loss_fn(score, label)\n",
    "        return loss, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "39f02757-1e70-4cd5-b950-5b41bdfa1b35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37535"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_dict = pickle.load(open(os.path.join(cfg.data_dir + '_train', \"word_dict.bin\"), \"rb\"))\n",
    "len(word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "550e0ec8-6366-441a-ad24-d23d09d9f5da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------\n",
      "Dict length: 37535\n",
      "Have words: 30808\n",
      "Missing rate: 0.17921939523111763\n"
     ]
    }
   ],
   "source": [
    "glove_emb = load_pretrain_emb(cfg.glove_path, word_dict, cfg.word_emb_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "79cbeb75-3edc-4dc5-848e-a6b7b805fd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_dict = pickle.load(open(os.path.join(cfg.data_dir + '_train', \"category_dict.bin\"), \"rb\"))\n",
    "subcategory_dict = pickle.load(open(os.path.join(cfg.data_dir + '_train', \"subcategory_dict.bin\"), \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0b1fc4f7-bb86-403e-8582-da8bcf6ad281",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NAML(\n",
       "  (news_encoder): NewsEncoder(\n",
       "    (embedding_matrix): Embedding(37536, 300, padding_idx=0)\n",
       "    (category_emb): Embedding(18, 100, padding_idx=0)\n",
       "    (category_dense): Linear(in_features=100, out_features=400, bias=True)\n",
       "    (subcategory_emb): Embedding(265, 100, padding_idx=0)\n",
       "    (subcategory_dense): Linear(in_features=100, out_features=400, bias=True)\n",
       "    (final_attn): AttentionPooling(\n",
       "      (att_fc1): Linear(in_features=400, out_features=200, bias=True)\n",
       "      (att_fc2): Linear(in_features=200, out_features=1, bias=True)\n",
       "    )\n",
       "    (cnn): Conv1d(300, 400, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (attn): AttentionPooling(\n",
       "      (att_fc1): Linear(in_features=400, out_features=200, bias=True)\n",
       "      (att_fc2): Linear(in_features=200, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (user_encoder): UserEncoder(\n",
       "    (attn): AttentionPooling(\n",
       "      (att_fc1): Linear(in_features=400, out_features=200, bias=True)\n",
       "      (att_fc2): Linear(in_features=200, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (loss_fn): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NAML(glove_emb, len(category_dict), len(subcategory_dict))\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0003)\n",
    "model = model.cuda()\n",
    "torch.set_grad_enabled(True)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "42ab74c3-be64-4f83-856c-e0eab5720c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc(y_true, y_hat):\n",
    "    y_hat = torch.argmax(y_hat, dim=-1)\n",
    "    tot = y_true.shape[0]\n",
    "    hit = torch.sum(y_true == y_hat)\n",
    "    return hit.data.float() * 1.0 / tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1490b1-e367-4e07-a690-06f59425b44c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1847it [02:01, 15.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2530.2322, device='cuda:0') tensor(818.2031, device='cuda:0')\n",
      "EPOCH: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1847it [01:57, 15.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2421.7759, device='cuda:0') tensor(871.0703, device='cuda:0')\n",
      "EPOCH: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1016it [01:05, 15.59it/s]"
     ]
    }
   ],
   "source": [
    "for ep in range(6):\n",
    "    loss = 0.0\n",
    "    accuary = 0.0\n",
    "    print(\"EPOCH: \" + str(ep))\n",
    "    for cnt, (_, log_ids, log_mask, input_ids, targets) in tqdm(enumerate(dataloader)):\n",
    "        log_ids = log_ids.cuda()\n",
    "        log_mask = log_mask.cuda()\n",
    "        input_ids = input_ids.cuda()\n",
    "        targets = targets.cuda()\n",
    "\n",
    "        bz_loss, y_hat = model(log_ids, log_mask, input_ids, targets)\n",
    "        loss += bz_loss.data.float()\n",
    "        accuary += acc(targets, y_hat)\n",
    "        optimizer.zero_grad()\n",
    "        bz_loss.backward()\n",
    "        optimizer.step()\n",
    "        # stop\n",
    "    print(loss, accuary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619ceb00-d40f-426c-94b3-3bd4646dcfa3",
   "metadata": {},
   "source": [
    "# Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "94ac9e3a-5b13-446f-bf6f-171c99d24cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d066192f-25d9-4a1a-b1c7-dab919a97df2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x231949bc4c0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0c734545-520a-4773-90d7-dc0cfa911166",
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = \"val\"\n",
    "data_dir = {\"train\": cfg.data_dir + '_train', \"val\": cfg.data_dir + '_val', \"test\": cfg.data_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e48bd4e7-628e-4a12-8a7e-78b5328a9b9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./data/MINDsmall_val'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_index = pickle.load(open(Path(data_dir[mode]) / \"news_dict.bin\", \"rb\"))\n",
    "news_input = pickle.load(open(Path(data_dir[mode]) / \"nltk_token_news.bin\", \"rb\"))\n",
    "data_dir[mode]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "39596d22-f1bc-4945-a650-c38372afeb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_dataset = NewsDataset(news_input)\n",
    "news_dataloader = DataLoader(news_dataset, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03a3ce4-9bea-4357-ac10-841486f56bcf",
   "metadata": {},
   "source": [
    "## news -> scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9142bf3b-4d4e-4094-91bb-5a024a647abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 510/510 [00:25<00:00, 20.23it/s]\n"
     ]
    }
   ],
   "source": [
    "news_scoring = []\n",
    "with torch.no_grad():\n",
    "    for input_ids in tqdm(news_dataloader):\n",
    "        input_ids = input_ids.cuda()\n",
    "        news_vec = model.news_encoder(input_ids)\n",
    "        news_vec = news_vec.to(torch.device(\"cpu\")).detach().numpy()\n",
    "        news_scoring.extend(news_vec)\n",
    "\n",
    "news_scoring = np.array(news_scoring)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502b6dbe-19ba-40a6-89be-ab292c9c0791",
   "metadata": {},
   "source": [
    "## val loader and compute score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8e664e0d-dae5-4cb9-b7f3-775b9939472f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValidDataset_PANEL_1(Dataset_PANEL_1):\n",
    "    def __init__(self, filename, news_index, news_score, cfg, neighbor_dict, news_graph):\n",
    "        super(Dataset_PANEL_1).__init__()\n",
    "        self.filename = filename\n",
    "        self.news_index = news_index\n",
    "        self.news_score = news_score\n",
    "        self.user_log_length = cfg.his_size\n",
    "        self.npratio = cfg.npratio\n",
    "        self.cfg = cfg\n",
    "        self.neighbor_dict = neighbor_dict\n",
    "        self.news_graph = news_graph\n",
    "        \n",
    "\n",
    "    def line_mapper(self, line):\n",
    "        line = line.strip().split('\\t')\n",
    "        click_docs = line[3].split()\n",
    "        \n",
    "        candidate_news = self.trans_to_nindex([i.split('-')[0] for i in line[4].split()])\n",
    "        labels = np.array([int(i.split('-')[1]) for i in line[4].split()])\n",
    "        \n",
    "        click_docs = self.trans_to_nindex(click_docs)\n",
    "\n",
    "        # build sub-graph\n",
    "        k_hops_click = self.build_k_hop(click_docs)\n",
    "        # print(max(k_hops_click))\n",
    "        \n",
    "        subemb = self.news_graph.x[k_hops_click]\n",
    "        sub_edge_index, sub_edge_attr = subgraph(k_hops_click, self.news_graph.edge_index, self.news_graph.edge_attr, \\\n",
    "                                                 relabel_nodes=True, num_nodes=self.news_graph.num_nodes)\n",
    "        sub_news_graph = Data(x=subemb, edge_index=sub_edge_index, edge_attr=sub_edge_attr)\n",
    "\n",
    "        \n",
    "        click_docs, log_mask = self.pad_to_fix_len(click_docs, self.user_log_length)\n",
    "        user_feature = self.news_score[click_docs]\n",
    "\n",
    "        news_feature = self.news_score[candidate_news]\n",
    "        \n",
    "        return sub_news_graph, torch.from_numpy(user_feature), torch.from_numpy(log_mask), \\\n",
    "        torch.from_numpy(news_feature), torch.from_numpy(labels)\n",
    "\n",
    "    def __iter__(self):\n",
    "        file_iter = open(self.filename)\n",
    "        return map(self.line_mapper, file_iter)\n",
    "\n",
    "# def my_collate(tuple_list):\n",
    "#     # dont know why cant push collate_fn to dataloader --> set batch_size = 1        \n",
    "#     # graph = [x[0] for x in tuple_list]\n",
    "#     his_vecs = torch.FloatTensor([x[1] for x in tuple_list])\n",
    "#     his_mask = torch.FloatTensor([x[2] for x in tuple_list])\n",
    "#     news_vecs = [x[3] for x in tuple_list]\n",
    "#     labels = [x[4] for x in tuple_list]\n",
    "#     return (his_vecs, his_mask, news_vecs, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "098ac2d6-3047-4e0d-8e5f-b33f0d19254e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('data/MINDsmall_val/behaviors.tsv')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_file = Path(data_dir[mode]) / f\"behaviors.tsv\"\n",
    "target_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0cc6c9c4-9065-4ddb-98db-af4381ba7d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_graph = torch.load(Path(data_dir[mode]) / \"nltk_news_graph.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1774a9e9-5684-4d1e-8618-5e154dd20523",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset = ValidDataset_PANEL_1(\n",
    "                filename=target_file,\n",
    "                news_index=news_index,\n",
    "                news_score=news_scoring,\n",
    "                cfg=cfg,\n",
    "                neighbor_dict=news_neighbors_dict,\n",
    "                news_graph=news_graph\n",
    ")\n",
    "valid_dataloader = GraphDataLoader(valid_dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0298f963-fe21-4f84-9291-1495c24e3783",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = iter(valid_dataloader)\n",
    "data_batch = next(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f69776be-92cd-461e-af23-09bc502fcd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "g, uf, lm, nf, l = data_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "093e3b3e-214a-4958-a7bd-dbe9c8f4f5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUC = []\n",
    "MRR = []\n",
    "nDCG5 = []\n",
    "nDCG10 = []\n",
    "\n",
    "def print_metrics(cnt, x):\n",
    "    print(cnt, x)\n",
    "\n",
    "def get_mean(arr):\n",
    "    return [np.array(i).mean() for i in arr]\n",
    "\n",
    "def get_sum(arr):\n",
    "    return [np.array(i).sum() for i in arr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c70af8bf-0d03-4900-bab4-7913cd5ab743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [0.5753862579422542, 0.2625937133328324, 0.2828842071558331, 0.34351409351621887]\n",
      "1000 [0.5760497182186805, 0.26369144615888385, 0.28378754472207063, 0.34498185971132383]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[80], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cnt, (_, log_vecs, log_mask, news_vecs, labels) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(valid_dataloader):\n\u001b[0;32m      2\u001b[0m     log_vecs \u001b[38;5;241m=\u001b[39m log_vecs\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m      3\u001b[0m     log_mask \u001b[38;5;241m=\u001b[39m log_mask\u001b[38;5;241m.\u001b[39mcuda()\n",
      "File \u001b[1;32mD:\\anaconda\\envs\\dangkh\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mD:\\anaconda\\envs\\dangkh\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mD:\\anaconda\\envs\\dangkh\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:32\u001b[0m, in \u001b[0;36m_IterableDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index:\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 32\u001b[0m         data\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset_iter\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m     34\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[57], line 28\u001b[0m, in \u001b[0;36mValidDataset_PANEL_1.line_mapper\u001b[1;34m(self, line)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# print(max(k_hops_click))\u001b[39;00m\n\u001b[0;32m     27\u001b[0m subemb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnews_graph\u001b[38;5;241m.\u001b[39mx[k_hops_click]\n\u001b[1;32m---> 28\u001b[0m sub_edge_index, sub_edge_attr \u001b[38;5;241m=\u001b[39m \u001b[43msubgraph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk_hops_click\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnews_graph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnews_graph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_attr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mrelabel_nodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_nodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnews_graph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_nodes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m sub_news_graph \u001b[38;5;241m=\u001b[39m Data(x\u001b[38;5;241m=\u001b[39msubemb, edge_index\u001b[38;5;241m=\u001b[39msub_edge_index, edge_attr\u001b[38;5;241m=\u001b[39msub_edge_attr)\n\u001b[0;32m     33\u001b[0m click_docs, log_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpad_to_fix_len(click_docs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_log_length)\n",
      "File \u001b[1;32mD:\\anaconda\\envs\\dangkh\\lib\\site-packages\\torch_geometric\\utils\\_subgraph.py:145\u001b[0m, in \u001b[0;36msubgraph\u001b[1;34m(subset, edge_index, edge_attr, relabel_nodes, num_nodes, return_edge_mask)\u001b[0m\n\u001b[0;32m    143\u001b[0m edge_mask \u001b[38;5;241m=\u001b[39m node_mask[edge_index[\u001b[38;5;241m0\u001b[39m]] \u001b[38;5;241m&\u001b[39m node_mask[edge_index[\u001b[38;5;241m1\u001b[39m]]\n\u001b[0;32m    144\u001b[0m edge_index \u001b[38;5;241m=\u001b[39m edge_index[:, edge_mask]\n\u001b[1;32m--> 145\u001b[0m edge_attr \u001b[38;5;241m=\u001b[39m \u001b[43medge_attr\u001b[49m\u001b[43m[\u001b[49m\u001b[43medge_mask\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m edge_attr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m relabel_nodes:\n\u001b[0;32m    148\u001b[0m     edge_index, _ \u001b[38;5;241m=\u001b[39m map_index(\n\u001b[0;32m    149\u001b[0m         edge_index\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[0;32m    150\u001b[0m         subset,\n\u001b[0;32m    151\u001b[0m         max_index\u001b[38;5;241m=\u001b[39mnum_nodes,\n\u001b[0;32m    152\u001b[0m         inclusive\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    153\u001b[0m     )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for cnt, (_, log_vecs, log_mask, news_vecs, labels) in enumerate(valid_dataloader):\n",
    "    log_vecs = log_vecs.cuda()\n",
    "    log_mask = log_mask.cuda()\n",
    "\n",
    "    user_vecs = model.user_encoder(log_vecs, log_mask).to(torch.device(\"cpu\")).detach().numpy()\n",
    "    labels = labels.to(torch.device(\"cpu\")).detach().numpy()\n",
    "\n",
    "    for user_vec, news_vec, label in zip(user_vecs, news_vecs, labels):\n",
    "        tmp = np.mean(label)\n",
    "        if tmp == 0 or tmp == 1:\n",
    "            continue\n",
    "\n",
    "        score = np.dot(news_vec, user_vec)\n",
    "        auc = roc_auc_score(label, score)\n",
    "        mrr = mrr_score(label, score)\n",
    "        ndcg5 = ndcg_score(label, score, k=5)\n",
    "        ndcg10 = ndcg_score(label, score, k=10)\n",
    "\n",
    "        AUC.append(auc)\n",
    "        MRR.append(mrr)\n",
    "        nDCG5.append(ndcg5)\n",
    "        nDCG10.append(ndcg10)\n",
    "\n",
    "    if cnt % 10000 == 0:\n",
    "        print_metrics(cnt, get_mean([AUC, MRR, nDCG5, nDCG10]))\n",
    "\n",
    "print_metrics(cnt, get_mean([AUC, MRR, nDCG5, nDCG10]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
