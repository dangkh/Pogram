{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9ab791d-14da-4b4d-b707-685d8e07a9cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\dangkh\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from src.config import TrainConfig \n",
    "from src.ultis import *\n",
    "from src.data_helper import prepare_preprocessed_data\n",
    "from src.data_load import *\n",
    "from src.metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00f99f0e-411a-4e2a-a607-e1eced00819e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "129e3122-9b9f-43c3-87f8-1364326cbc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "device: torch.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f84c11bf-8379-4420-8913-c4636faaddc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = TrainConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4228444-36ca-48bd-831b-673e449dc985",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_random_seed(cfg.random_seed)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ddb2613-8bfb-4ab3-a65a-bff0bab3933a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-13 12:18:35,938 INFO Start\n",
      "2024-10-13 12:18:35,939 INFO Prepare the dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] All preprocessed files exist.\n",
      "[val] All preprocessed files exist.\n",
      "[train] All graphs exist !\n",
      "[val] All graphs exist !\n",
      "[train] Start to process neighbors list\n",
      "[train] All news Neighbor dict exist !\n",
      "[val] Start to process neighbors list\n",
      "[val] All news Neighbor dict exist !\n",
      "[train] Entity graph exists!\n",
      "[val] Entity graph exists!\n",
      "[train] Start to process neighbors list\n",
      "[train] All entity Neighbor dict exist !\n",
      "[val] Start to process neighbors list\n",
      "[val] All entity Neighbor dict exist !\n"
     ]
    }
   ],
   "source": [
    "logging.info(\"Start\")\n",
    "\"\"\"\n",
    "0. Definite Parameters & Functions\n",
    "\n",
    "\"\"\"\n",
    "logging.info(\"Prepare the dataset\")\n",
    "prepare_preprocessed_data(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d675ac1-49b8-40d9-bb0f-72eb155356b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mode='train'\n",
    "data_dir = {\"train\": cfg.data_dir + '_train', \"val\": cfg.data_dir + '_val', \"test\": cfg.data_dir}\n",
    "\n",
    "# ------------- load news.tsv-------------\n",
    "news_index = pickle.load(open(Path(data_dir[mode]) / \"news_dict.bin\", \"rb\"))\n",
    "\n",
    "news_input = pickle.load(open(Path(data_dir[mode]) / \"nltk_token_news.bin\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be0621d9-4d85-45e6-b060-60f0289ce1de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] News Graph Info: Data(x=[51283, 22], edge_index=[2, 1171966], edge_attr=[1171966], num_nodes=51283)\n"
     ]
    }
   ],
   "source": [
    "target_file = Path(data_dir[mode]) / f\"behaviors_np{cfg.npratio}_0.tsv\"\n",
    "news_graph = torch.load(Path(data_dir[mode]) / \"nltk_news_graph.pt\")\n",
    "\n",
    "if cfg.directed is False:\n",
    "    news_graph.edge_index, news_graph.edge_attr = to_undirected(news_graph.edge_index, news_graph.edge_attr)\n",
    "print(f\"[{mode}] News Graph Info: {news_graph}\")\n",
    "\n",
    "news_neighbors_dict = pickle.load(open(Path(data_dir[mode]) / \"news_neighbor_dict.bin\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e03912b9-d773-4763-90ca-b435671b6079",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data, Batch\n",
    "from torch_geometric.utils import subgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "274d0b02-e5f5-469a-8525-3cc23f170700",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.utils.data import IterableDataset\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader as GraphDataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd74e17d-6309-4a94-a130-9f5108f4be3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset_PANEL_1(IterableDataset):\n",
    "    def __init__(self, filename, news_index, news_combined, cfg, neighbor_dict, news_graph):\n",
    "        super(Dataset_PANEL_1).__init__()\n",
    "        self.filename = filename\n",
    "        self.news_index = news_index\n",
    "        self.news_combined = news_combined\n",
    "        self.user_log_length = cfg.his_size\n",
    "        self.npratio = cfg.npratio\n",
    "        self.cfg = cfg\n",
    "        self.neighbor_dict = neighbor_dict\n",
    "        self.news_graph = news_graph\n",
    "\n",
    "    def trans_to_nindex(self, nids):\n",
    "        return [self.news_index[i] if i in self.news_index else 0 for i in nids]\n",
    "\n",
    "    def pad_to_fix_len(self, x, fix_length, padding_front=True, padding_value=0):\n",
    "        if padding_front:\n",
    "            pad_x = [padding_value] * (fix_length - len(x)) + x[-fix_length:]\n",
    "            mask = [0] * (fix_length - len(x)) + [1] * min(fix_length, len(x))\n",
    "        else:\n",
    "            pad_x = x[-fix_length:] + [padding_value] * (fix_length - len(x))\n",
    "            mask = [1] * min(fix_length, len(x)) + [0] * (fix_length - len(x))\n",
    "        return pad_x, np.array(mask, dtype='float32')\n",
    "\n",
    "    def build_k_hop(self, click_doc):\n",
    "        click_idx = [x for x in click_doc]\n",
    "        source_idx = [x for x in click_doc]\n",
    "        for _ in range(self.cfg.k_hops) :\n",
    "            current_hop_idx = []\n",
    "            for news_idx in source_idx:\n",
    "                current_hop_idx.extend(self.neighbor_dict[news_idx][:self.cfg.num_neighbors])\n",
    "            source_idx = current_hop_idx\n",
    "            click_idx.extend(current_hop_idx)\n",
    "        return list(set(click_idx))\n",
    "        \n",
    "\n",
    "    def line_mapper(self, line):\n",
    "        line = line.strip().split('\\t')\n",
    "        click_docs = line[3].split()\n",
    "        sess_pos = line[4].split()\n",
    "        sess_neg = line[5].split()\n",
    "        click_docs = self.trans_to_nindex(click_docs)\n",
    "\n",
    "        # build sub-graph\n",
    "        k_hops_click = self.build_k_hop(click_docs)\n",
    "        \n",
    "        subemb = self.news_graph.x[k_hops_click]\n",
    "        sub_edge_index, sub_edge_attr = subgraph(k_hops_click, self.news_graph.edge_index, self.news_graph.edge_attr, \\\n",
    "                                                 relabel_nodes=True, num_nodes=self.news_graph.num_nodes)\n",
    "        sub_news_graph = Data(x=subemb, edge_index=sub_edge_index, edge_attr=sub_edge_attr)\n",
    "\n",
    "        \n",
    "        click_docs, log_mask = self.pad_to_fix_len(click_docs, self.user_log_length)\n",
    "        user_feature = self.news_combined[click_docs]\n",
    "\n",
    "        pos = self.trans_to_nindex(sess_pos)\n",
    "        neg = self.trans_to_nindex(sess_neg)\n",
    "\n",
    "        label = random.randint(0, self.npratio)\n",
    "        sample_news = neg[:label] + pos + neg[label:]\n",
    "        news_feature = self.news_combined[sample_news]\n",
    "        return sub_news_graph, torch.from_numpy(user_feature), torch.from_numpy(log_mask), \\\n",
    "        torch.from_numpy(news_feature), torch.tensor(label)\n",
    "\n",
    "    def __iter__(self):\n",
    "        file_iter = open(self.filename)\n",
    "        return map(self.line_mapper, file_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c08d7aa-6ec7-4d55-beff-7eafee12f3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_type(x):\n",
    "    if isinstance(x, np.ndarray):\n",
    "        print(\"This is a NumPy array.\")\n",
    "    elif isinstance(x, list):\n",
    "        print(\"This is a Python list.\")\n",
    "    elif isinstance(x, torch.Tensor):\n",
    "        print(\"This is a PyTorch tensor.\")\n",
    "    else:\n",
    "        print(\"Unknown type.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4902e54f-cfe0-4e46-a1d2-9f4fb94a3adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset_PANEL_1(\n",
    "                filename=target_file,\n",
    "                news_index=news_index,\n",
    "                news_combined=news_input,\n",
    "                cfg=cfg,\n",
    "                neighbor_dict=news_neighbors_dict,\n",
    "                news_graph=news_graph\n",
    ")\n",
    "dataloader = GraphDataLoader(dataset, batch_size=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d9505e17-0339-4677-9bcf-3b1fddee7b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterator = iter(dataloader)\n",
    "# data_batch = next(iterator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bff8d6cd-6ce2-43e8-8543-22868f441484",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sub_news_graph, user_feature, log_mask, news_feature, label = data_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5b43a080-50ab-4d23-aaff-b2beb4f5ad64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 50, 22])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "84502714-78b3-4e44-a5df-530935c54695",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionPooling(nn.Module):\n",
    "    def __init__(self, emb_size, hidden_size):\n",
    "        super(AttentionPooling, self).__init__()\n",
    "        self.att_fc1 = nn.Linear(emb_size, hidden_size)\n",
    "        self.att_fc2 = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x, attn_mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: batch_size, candidate_size, emb_dim\n",
    "            attn_mask: batch_size, candidate_size\n",
    "        Returns:\n",
    "            (shape) batch_size, emb_dim\n",
    "        \"\"\"\n",
    "        e = self.att_fc1(x)\n",
    "        e = nn.Tanh()(e)\n",
    "        alpha = self.att_fc2(e)\n",
    "        alpha = torch.exp(alpha)\n",
    "\n",
    "        if attn_mask is not None:\n",
    "            alpha = alpha * attn_mask.unsqueeze(2)\n",
    "\n",
    "        alpha = alpha / (torch.sum(alpha, dim=1, keepdim=True) + 1e-8)\n",
    "        x = torch.bmm(x.permute(0, 2, 1), alpha).squeeze(dim=-1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, d_k):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "        self.d_k = d_k\n",
    "\n",
    "    def forward(self, Q, K, V, attn_mask=None):\n",
    "        '''\n",
    "            Q: batch_size, n_head, candidate_num, d_k\n",
    "            K: batch_size, n_head, candidate_num, d_k\n",
    "            V: batch_size, n_head, candidate_num, d_v\n",
    "            attn_mask: batch_size, n_head, candidate_num\n",
    "            Return: batch_size, n_head, candidate_num, d_v\n",
    "        '''\n",
    "        scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(self.d_k)\n",
    "        scores = torch.exp(scores)\n",
    "\n",
    "        if attn_mask is not None:\n",
    "            scores = scores * attn_mask.unsqueeze(dim=-2)\n",
    "\n",
    "        attn = scores / (torch.sum(scores, dim=-1, keepdim=True) + 1e-8)\n",
    "        context = torch.matmul(attn, V)\n",
    "        return context\n",
    "\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_k, d_v):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "\n",
    "        self.W_Q = nn.Linear(d_model, d_k * n_heads)\n",
    "        self.W_K = nn.Linear(d_model, d_k * n_heads)\n",
    "        self.W_V = nn.Linear(d_model, d_v * n_heads)\n",
    "\n",
    "        self.scaled_dot_product_attn = ScaledDotProductAttention(self.d_k)\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight, gain=1)\n",
    "\n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        '''\n",
    "            Q: batch_size, candidate_num, d_model\n",
    "            K: batch_size, candidate_num, d_model\n",
    "            V: batch_size, candidate_num, d_model\n",
    "            mask: batch_size, candidate_num\n",
    "        '''\n",
    "        batch_size = Q.shape[0]\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(dim=1).expand(-1, self.n_heads, -1)\n",
    "\n",
    "        q_s = self.W_Q(Q).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        k_s = self.W_K(K).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        v_s = self.W_V(V).view(batch_size, -1, self.n_heads, self.d_v).transpose(1, 2)\n",
    "\n",
    "        context = self.scaled_dot_product_attn(q_s, k_s, v_s, mask)\n",
    "        output = context.transpose(1, 2).contiguous().view(batch_size, -1, self.n_heads * self.d_v)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1e4eb076-b86c-4d29-997e-59ab12de9091",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsEncoder(nn.Module):\n",
    "    def __init__(self, embedding_matrix, num_category, num_subcategory):\n",
    "        super(NewsEncoder, self).__init__()\n",
    "        self.embedding_matrix = embedding_matrix\n",
    "        self.drop_rate = 0.2\n",
    "        self.num_words_title = 20\n",
    "        self.use_category = True\n",
    "        self.use_subcategory = True\n",
    "        category_emb_dim = 100\n",
    "        news_dim = 400\n",
    "        news_query_vector_dim = 200\n",
    "        word_embedding_dim = 300\n",
    "        self.category_emb = nn.Embedding(num_category + 1, category_emb_dim, padding_idx=0)\n",
    "        self.category_dense = nn.Linear(category_emb_dim, news_dim)\n",
    "        self.subcategory_emb = nn.Embedding(num_subcategory + 1, category_emb_dim, padding_idx=0)\n",
    "        self.subcategory_dense = nn.Linear(category_emb_dim, news_dim)\n",
    "        self.final_attn = AttentionPooling(news_dim, news_query_vector_dim)\n",
    "        self.cnn = nn.Conv1d(\n",
    "            in_channels=word_embedding_dim,\n",
    "            out_channels=news_dim,\n",
    "            kernel_size=3,\n",
    "            padding=1\n",
    "        )\n",
    "        self.attn = AttentionPooling(news_dim, news_query_vector_dim)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        '''\n",
    "            x: batch_size, word_num\n",
    "            mask: batch_size, word_num\n",
    "        '''\n",
    "        title = torch.narrow(x, -1, 0, self.num_words_title).long()\n",
    "        word_vecs = F.dropout(self.embedding_matrix(title),\n",
    "                              p=self.drop_rate,\n",
    "                              training=self.training)\n",
    "        context_word_vecs = self.cnn(word_vecs.transpose(1, 2)).transpose(1, 2)\n",
    "        \n",
    "        title_vecs = self.attn(context_word_vecs, mask)\n",
    "        all_vecs = [title_vecs]\n",
    "\n",
    "        start = self.num_words_title\n",
    "        if self.use_category:\n",
    "            category = torch.narrow(x, -1, start, 1).squeeze(dim=-1).long()\n",
    "            category_vecs = self.category_dense(self.category_emb(category))\n",
    "            all_vecs.append(category_vecs)\n",
    "            start += 1\n",
    "        if self.use_subcategory:\n",
    "            subcategory = torch.narrow(x, -1, start, 1).squeeze(dim=-1).long()\n",
    "            subcategory_vecs = self.subcategory_dense(self.subcategory_emb(subcategory))\n",
    "            all_vecs.append(subcategory_vecs)\n",
    "\n",
    "        if len(all_vecs) == 1:\n",
    "            news_vecs = all_vecs[0]\n",
    "        else:\n",
    "            all_vecs = torch.stack(all_vecs, dim=1)\n",
    "            \n",
    "            news_vecs = self.final_attn(all_vecs)\n",
    "        return news_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3f05fd2f-652d-4eaa-8da0-85f503ff1447",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class UserEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UserEncoder, self).__init__()\n",
    "        news_dim = 400\n",
    "        user_query_vector_dim = 200\n",
    "        self.user_log_length = 100\n",
    "        self.user_log_mask = False\n",
    "        self.attn = AttentionPooling(news_dim, user_query_vector_dim)\n",
    "        self.pad_doc = nn.Parameter(torch.empty(1, news_dim).uniform_(-1, 1)).type(torch.FloatTensor)\n",
    "\n",
    "    def forward(self, news_vecs, log_mask=None):\n",
    "        '''\n",
    "            news_vecs: batch_size, history_num, news_dim\n",
    "            log_mask: batch_size, history_num\n",
    "        '''\n",
    "        bz = news_vecs.shape[0]\n",
    "        if self.user_log_mask:\n",
    "            user_vec = self.attn(news_vecs, log_mask)\n",
    "        else:\n",
    "            padding_doc = self.pad_doc.unsqueeze(dim=0).expand(bz, self.user_log_length, -1)\n",
    "            news_vecs = news_vecs * log_mask.unsqueeze(dim=-1) + padding_doc * (1 - log_mask.unsqueeze(dim=-1))\n",
    "            user_vec = self.attn(news_vecs)\n",
    "        return user_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ddaa5be1-8b93-41e0-a685-38c74b4ff8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NAML(torch.nn.Module):\n",
    "    def __init__(self, embedding_matrix, num_category, num_subcategory, **kwargs):\n",
    "        super(NAML, self).__init__()\n",
    "        pretrained_word_embedding = torch.from_numpy(embedding_matrix).float()\n",
    "        word_embedding = nn.Embedding.from_pretrained(pretrained_word_embedding,\n",
    "                                                      freeze=False,\n",
    "                                                      padding_idx=0)\n",
    "\n",
    "        self.news_encoder = NewsEncoder( word_embedding, num_category, num_subcategory)\n",
    "        self.user_encoder = UserEncoder()\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        self.npratio = 4\n",
    "        self.news_dim = 400\n",
    "        self.user_log_length = 100\n",
    "\n",
    "    def forward(self, history, history_mask, candidate, label):\n",
    "        '''\n",
    "            history: batch_size, history_length, num_word_title\n",
    "            history_mask: batch_size, history_length\n",
    "            candidate: batch_size, 1+K, num_word_title\n",
    "            label: batch_size, 1+K\n",
    "        '''\n",
    "        num_words = history.shape[-1]\n",
    "        candidate_news = candidate.reshape(-1, num_words)\n",
    "        candidate_news_vecs = self.news_encoder(candidate_news).reshape(-1, 1 + self.npratio, self.news_dim)\n",
    "        history_news = history.reshape(-1, num_words)\n",
    "        history_news_vecs = self.news_encoder(history_news).reshape(-1, self.user_log_length, self.news_dim)\n",
    "        user_vec = self.user_encoder(history_news_vecs, history_mask)\n",
    "        score = torch.bmm(candidate_news_vecs, user_vec.unsqueeze(dim=-1)).squeeze(dim=-1)\n",
    "        loss = self.loss_fn(score, label)\n",
    "        return loss, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "39f02757-1e70-4cd5-b950-5b41bdfa1b35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37535"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_dict = pickle.load(open(os.path.join(cfg.data_dir + '_train', \"word_dict.bin\"), \"rb\"))\n",
    "len(word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "550e0ec8-6366-441a-ad24-d23d09d9f5da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------\n",
      "Dict length: 37535\n",
      "Have words: 30808\n",
      "Missing rate: 0.17921939523111763\n"
     ]
    }
   ],
   "source": [
    "glove_emb = load_pretrain_emb(cfg.glove_path, word_dict, cfg.word_emb_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "79cbeb75-3edc-4dc5-848e-a6b7b805fd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_dict = pickle.load(open(os.path.join(cfg.data_dir + '_train', \"category_dict.bin\"), \"rb\"))\n",
    "subcategory_dict = pickle.load(open(os.path.join(cfg.data_dir + '_train', \"subcategory_dict.bin\"), \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0b1fc4f7-bb86-403e-8582-da8bcf6ad281",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NAML(\n",
       "  (news_encoder): NewsEncoder(\n",
       "    (embedding_matrix): Embedding(37536, 300, padding_idx=0)\n",
       "    (category_emb): Embedding(18, 100, padding_idx=0)\n",
       "    (category_dense): Linear(in_features=100, out_features=400, bias=True)\n",
       "    (subcategory_emb): Embedding(265, 100, padding_idx=0)\n",
       "    (subcategory_dense): Linear(in_features=100, out_features=400, bias=True)\n",
       "    (final_attn): AttentionPooling(\n",
       "      (att_fc1): Linear(in_features=400, out_features=200, bias=True)\n",
       "      (att_fc2): Linear(in_features=200, out_features=1, bias=True)\n",
       "    )\n",
       "    (cnn): Conv1d(300, 400, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (attn): AttentionPooling(\n",
       "      (att_fc1): Linear(in_features=400, out_features=200, bias=True)\n",
       "      (att_fc2): Linear(in_features=200, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (user_encoder): UserEncoder(\n",
       "    (attn): AttentionPooling(\n",
       "      (att_fc1): Linear(in_features=400, out_features=200, bias=True)\n",
       "      (att_fc2): Linear(in_features=200, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (loss_fn): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NAML(glove_emb, len(category_dict), len(subcategory_dict))\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0003)\n",
    "model = model.cuda()\n",
    "torch.set_grad_enabled(True)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "be1490b1-e367-4e07-a690-06f59425b44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for ep in range(6):\n",
    "#     loss = 0.0\n",
    "#     accuary = 0.0\n",
    "#     print(\"EPOCH: \" + str(ep))\n",
    "#     for cnt, (log_ids, log_mask, input_ids, targets) in tqdm(enumerate(dataloader)):\n",
    "#         log_ids = log_ids.cuda()\n",
    "#         log_mask = log_mask.cuda()\n",
    "#         input_ids = input_ids.cuda()\n",
    "#         targets = targets.cuda()\n",
    "\n",
    "#         bz_loss, y_hat = model(log_ids, log_mask, input_ids, targets)\n",
    "#         loss += bz_loss.data.float()\n",
    "#         accuary += acc(targets, y_hat)\n",
    "#         optimizer.zero_grad()\n",
    "#         bz_loss.backward()\n",
    "#         optimizer.step()\n",
    "#         # stop\n",
    "#     print(loss, accuary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619ceb00-d40f-426c-94b3-3bd4646dcfa3",
   "metadata": {},
   "source": [
    "# Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "94ac9e3a-5b13-446f-bf6f-171c99d24cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d066192f-25d9-4a1a-b1c7-dab919a97df2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x21dfcf52070>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0c734545-520a-4773-90d7-dc0cfa911166",
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = \"val\"\n",
    "data_dir = {\"train\": cfg.data_dir + '_train', \"val\": cfg.data_dir + '_val', \"test\": cfg.data_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e48bd4e7-628e-4a12-8a7e-78b5328a9b9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./data/MINDsmall_val'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_index = pickle.load(open(Path(data_dir[mode]) / \"news_dict.bin\", \"rb\"))\n",
    "news_input = pickle.load(open(Path(data_dir[mode]) / \"nltk_token_news.bin\", \"rb\"))\n",
    "data_dir[mode]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "39596d22-f1bc-4945-a650-c38372afeb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_dataset = NewsDataset(news_input)\n",
    "news_dataloader = DataLoader(news_dataset, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03a3ce4-9bea-4357-ac10-841486f56bcf",
   "metadata": {},
   "source": [
    "## news -> scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9142bf3b-4d4e-4094-91bb-5a024a647abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 510/510 [00:28<00:00, 18.21it/s]\n"
     ]
    }
   ],
   "source": [
    "news_scoring = []\n",
    "with torch.no_grad():\n",
    "    for input_ids in tqdm(news_dataloader):\n",
    "        input_ids = input_ids.cuda()\n",
    "        news_vec = model.news_encoder(input_ids)\n",
    "        news_vec = news_vec.to(torch.device(\"cpu\")).detach().numpy()\n",
    "        news_scoring.extend(news_vec)\n",
    "\n",
    "news_scoring = np.array(news_scoring)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502b6dbe-19ba-40a6-89be-ab292c9c0791",
   "metadata": {},
   "source": [
    "## val loader and compute score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e664e0d-dae5-4cb9-b7f3-775b9939472f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValidDataset_PANEL_1(Dataset_PANEL_1):\n",
    "    def __init__(self, filename, news_index, news_combined, cfg, neighbor_dict, news_graph):\n",
    "        super(Dataset_PANEL_1).__init__()\n",
    "        self.filename = filename\n",
    "        self.news_index = news_index\n",
    "        self.news_combined = news_combined\n",
    "        self.user_log_length = cfg.his_size\n",
    "        self.npratio = cfg.npratio\n",
    "        self.cfg = cfg\n",
    "        self.neighbor_dict = neighbor_dict\n",
    "        self.news_graph = news_graph\n",
    "\n",
    "    def build_k_hop(self, click_doc):\n",
    "        click_idx = [x for x in click_doc]\n",
    "        source_idx = [x for x in click_doc]\n",
    "        for _ in range(self.cfg.k_hops) :\n",
    "            current_hop_idx = []\n",
    "            for news_idx in source_idx:\n",
    "                current_hop_idx.extend(self.neighbor_dict[news_idx][:self.cfg.num_neighbors])\n",
    "            source_idx = current_hop_idx\n",
    "            click_idx.extend(current_hop_idx)\n",
    "        return list(set(click_idx))\n",
    "        \n",
    "\n",
    "    def line_mapper(self, line):\n",
    "        line = line.strip().split('\\t')\n",
    "        click_docs = line[3].split()\n",
    "        sess_pos = line[4].split()\n",
    "        sess_neg = line[5].split()\n",
    "        click_docs = self.trans_to_nindex(click_docs)\n",
    "\n",
    "        # build sub-graph\n",
    "        k_hops_click = self.build_k_hop(click_docs)\n",
    "        \n",
    "        subemb = self.news_graph.x[k_hops_click]\n",
    "        sub_edge_index, sub_edge_attr = subgraph(k_hops_click, self.news_graph.edge_index, self.news_graph.edge_attr, \\\n",
    "                                                 relabel_nodes=True, num_nodes=self.news_graph.num_nodes)\n",
    "        sub_news_graph = Data(x=subemb, edge_index=sub_edge_index, edge_attr=sub_edge_attr)\n",
    "\n",
    "        \n",
    "        click_docs, log_mask = self.pad_to_fix_len(click_docs, self.user_log_length)\n",
    "        user_feature = self.news_combined[click_docs]\n",
    "\n",
    "        pos = self.trans_to_nindex(sess_pos)\n",
    "        neg = self.trans_to_nindex(sess_neg)\n",
    "\n",
    "        label = random.randint(0, self.npratio)\n",
    "        sample_news = neg[:label] + pos + neg[label:]\n",
    "        news_feature = self.news_combined[sample_news]\n",
    "        return sub_news_graph, torch.from_numpy(user_feature), torch.from_numpy(log_mask), \\\n",
    "        torch.from_numpy(news_feature), torch.tensor(label)\n",
    "\n",
    "    def __iter__(self):\n",
    "        file_iter = open(self.filename)\n",
    "        return map(self.line_mapper, file_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "098ac2d6-3047-4e0d-8e5f-b33f0d19254e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('data/MINDsmall_val/behaviors.tsv')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_file = Path(data_dir[mode]) / f\"behaviors.tsv\"\n",
    "target_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1774a9e9-5684-4d1e-8618-5e154dd20523",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ValidDataset_PANEL_1(\n",
    "                filename=target_file,\n",
    "                news_index=news_index,\n",
    "                news_combined=news_input,\n",
    "                cfg=cfg,\n",
    "                neighbor_dict=news_neighbors_dict,\n",
    "                news_graph=news_graph\n",
    ")\n",
    "dataloader = GraphDataLoader(dataset, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0298f963-fe21-4f84-9291-1495c24e3783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterator = iter(dataloader)\n",
    "# data_batch = next(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "093e3b3e-214a-4958-a7bd-dbe9c8f4f5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUC = []\n",
    "MRR = []\n",
    "nDCG5 = []\n",
    "nDCG10 = []"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
