{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9ab791d-14da-4b4d-b707-685d8e07a9cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\dangkh\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from src.config import TrainConfig \n",
    "from src.ultis import *\n",
    "from src.data_helper import prepare_preprocessed_data\n",
    "from src.data_load import *\n",
    "from src.metrics import *\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch_geometric.data import Data, Batch\n",
    "from torch_geometric.utils import subgraph\n",
    "\n",
    "from torch.utils.data import IterableDataset\n",
    "from torch_geometric.loader import DataLoader as GraphDataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "129e3122-9b9f-43c3-87f8-1364326cbc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = TrainConfig\n",
    "device: torch.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device: torch.device = torch.device(\"cpu\")\n",
    "\n",
    "set_random_seed(cfg.random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03fe6bd8-e3a0-4874-ac45-9a037eaaf741",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = []\n",
    "len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b273795c-1df4-4f8b-894a-20f432b3be1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.history_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ddb2613-8bfb-4ab3-a65a-bff0bab3933a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-24 14:20:07,570 INFO Start\n",
      "2024-11-24 14:20:07,570 INFO Prepare the dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target_file is not exist. New behavior file in ./data/MINDsmall_train\\behaviors_np4_0.tsv\n",
      "./data/MINDsmall_train\\behaviors_np4_0.tsv ./data/MINDsmall_train\\behaviors.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "156965it [00:02, 55245.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train]Writing files...\n",
      "Target_file is not exist. New behavior file in ./data/MINDsmall_val\\behaviors_np4_0.tsv\n",
      "./data/MINDsmall_val\\behaviors_np4_0.tsv ./data/MINDsmall_val\\behaviors.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "73152it [00:00, 154384.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val]Writing files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[train]Processing raw news: 100%|██████████████████████████████████████████████| 51282/51282 [00:05<00:00, 8644.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing parsed news: 100%|████████████████████████████████████████████████| 51282/51282 [00:00<00:00, 173250.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Glove token preprocess finish.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[val]Processing raw news: 100%|████████████████████████████████████████████████| 42416/42416 [00:04<00:00, 9613.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing parsed news: 100%|████████████████████████████████████████████████| 65238/65238 [00:00<00:00, 174433.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Glove token preprocess finish.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[train] Processing behaviors news to News Graph: 100%|█████████████████████| 156965/156965 [00:00<00:00, 261608.37it/s]\n",
      "Processing news edge list: 100%|█████████████████████████████████████████████| 48304/48304 [00:00<00:00, 280790.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[51283, 22], edge_index=[2, 632044], edge_attr=[632044], num_nodes=51283)\n",
      "[train] Finish News Graph Construction, \n",
      "Graph Path: data\\MINDsmall_train\\nltk_news_graph.pt \n",
      "Graph Info: Data(x=[51283, 22], edge_index=[2, 632044], edge_attr=[632044], num_nodes=51283)\n",
      "[val] Finish nltk News Graph Construction, \n",
      "Graph Path: data\\MINDsmall_val\\nltk_news_graph.pt\n",
      "Graph Info: Data(x=[65239, 22], edge_index=[2, 632044], edge_attr=[632044], num_nodes=65239)\n",
      "[train] Start to process neighbors list\n",
      "[train] Finish news Neighbor dict \n",
      "Dict Path: data\\MINDsmall_train\\news_neighbor_dict.bin, \n",
      "Weight Dict: data\\MINDsmall_train\\news_weights_dict.bin\n",
      "[val] Start to process neighbors list\n",
      "[val] Finish news Neighbor dict \n",
      "Dict Path: data\\MINDsmall_val\\news_neighbor_dict.bin, \n",
      "Weight Dict: data\\MINDsmall_val\\news_weights_dict.bin\n",
      "news_graph, Data(x=[51283, 22], edge_index=[2, 632044], edge_attr=[632044], num_nodes=51283)\n",
      "entity_indices,  (51283, 5)\n",
      "[train] Finish Entity Graph Construction, \n",
      " Graph Path: data\\MINDsmall_train\\entity_graph.pt \n",
      "Graph Info: Data(x=[14991], edge_index=[2, 296953], edge_attr=[296953], num_nodes=14991)\n",
      "[val] Finish Entity Graph Construction, \n",
      " Graph Path: data\\MINDsmall_val\\entity_graph.pt \n",
      "Graph Info: Data(x=[17586], edge_index=[2, 296953], edge_attr=[296953], num_nodes=17586)\n",
      "[train] Start to process neighbors list\n",
      "[train] Finish entity Neighbor dict \n",
      "Dict Path: data\\MINDsmall_train\\entity_neighbor_dict.bin, \n",
      "Weight Dict: data\\MINDsmall_train\\entity_weights_dict.bin\n",
      "[val] Start to process neighbors list\n",
      "[val] Finish entity Neighbor dict \n",
      "Dict Path: data\\MINDsmall_val\\entity_neighbor_dict.bin, \n",
      "Weight Dict: data\\MINDsmall_val\\entity_weights_dict.bin\n"
     ]
    }
   ],
   "source": [
    "logging.info(\"Start\")\n",
    "\"\"\"\n",
    "0. Definite Parameters & Functions\n",
    "\n",
    "\"\"\"\n",
    "logging.info(\"Prepare the dataset\")\n",
    "prepare_preprocessed_data(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d675ac1-49b8-40d9-bb0f-72eb155356b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mode='train'\n",
    "data_dir = {\"train\": cfg.data_dir + '_train', \"val\": cfg.data_dir + '_val', \"test\": cfg.data_dir}\n",
    "\n",
    "# ------------- load news.tsv-------------\n",
    "news_index = pickle.load(open(Path(data_dir[mode]) / \"news_dict.bin\", \"rb\"))\n",
    "\n",
    "news_input = pickle.load(open(Path(data_dir[mode]) / \"nltk_token_news.bin\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb981499-4558-48d4-8ccb-bef237a77c66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51283, 22)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be0621d9-4d85-45e6-b060-60f0289ce1de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] News Graph Info: Data(x=[51283, 22], edge_index=[2, 1171966], edge_attr=[1171966], num_nodes=51283)\n"
     ]
    }
   ],
   "source": [
    "target_file = Path(data_dir[mode]) / f\"behaviors_np{cfg.npratio}_0.tsv\"\n",
    "news_graph = torch.load(Path(data_dir[mode]) / \"nltk_news_graph.pt\")\n",
    "\n",
    "if cfg.directed is False:\n",
    "    news_graph.edge_index, news_graph.edge_attr = to_undirected(news_graph.edge_index, news_graph.edge_attr)\n",
    "print(f\"[{mode}] News Graph Info: {news_graph}\")\n",
    "\n",
    "news_neighbors_dict = pickle.load(open(Path(data_dir[mode]) / \"news_neighbor_dict.bin\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd74e17d-6309-4a94-a130-9f5108f4be3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset_PANEL_1(Dataset):\n",
    "    def __init__(self, filename, news_index, news_combined, cfg, neighbor_dict, news_graph):\n",
    "        super(Dataset_PANEL_1).__init__()\n",
    "        self.filename = filename\n",
    "        self.news_index = news_index\n",
    "        self.news_combined = news_combined\n",
    "        self.user_log_length = cfg.his_size\n",
    "        self.npratio = cfg.npratio\n",
    "        self.cfg = cfg\n",
    "        self.neighbor_dict = neighbor_dict\n",
    "        self.news_graph = news_graph\n",
    "        self.news_graph.x = self.news_graph.x.float()\n",
    "        self.prepare()\n",
    "\n",
    "    def trans_to_nindex(self, nids):\n",
    "        return [self.news_index[i] if i in self.news_index else 0 for i in nids]\n",
    "\n",
    "    def pad_to_fix_len(self, x, fix_length, padding_front=True, padding_value=0):\n",
    "        if padding_front:\n",
    "            pad_x = [padding_value] * (fix_length - len(x)) + x[-fix_length:]\n",
    "            mask = [0] * (fix_length - len(x)) + [1] * min(fix_length, len(x))\n",
    "        else:\n",
    "            pad_x = x[-fix_length:] + [padding_value] * (fix_length - len(x))\n",
    "            mask = [1] * min(fix_length, len(x)) + [0] * (fix_length - len(x))\n",
    "        return pad_x, np.array(mask, dtype='float32')\n",
    "\n",
    "    def build_k_hop(self, click_doc):\n",
    "        click_idx = [x for x in click_doc]\n",
    "        source_idx = [x for x in click_doc]\n",
    "        for _ in range(self.cfg.k_hops) :\n",
    "            current_hop_idx = []\n",
    "            for news_idx in source_idx:\n",
    "                current_hop_idx.extend(self.neighbor_dict[news_idx][:self.cfg.num_neighbors])\n",
    "            source_idx = current_hop_idx\n",
    "            click_idx.extend(current_hop_idx)\n",
    "        return list(set(click_idx))\n",
    "        \n",
    "    def prepare(self):\n",
    "        self.preprocessDT = []\n",
    "        with open(self.filename) as f:\n",
    "            for line in tqdm(f):\n",
    "                g, dt = self.line_mapper(line)\n",
    "                if len(g) == 0:\n",
    "                    continue\n",
    "                self.preprocessDT.append([g,dt])\n",
    "                if len(self.preprocessDT) > 10000:\n",
    "                    break\n",
    "    \n",
    "    def line_mapper(self, line):\n",
    "        line = line.strip().split('\\t')\n",
    "        click_docs = line[3].split()\n",
    "        sess_pos = line[4].split()\n",
    "        sess_neg = line[5].split()\n",
    "        click_docs = self.trans_to_nindex(click_docs)\n",
    "\n",
    "        # build sub-graph\n",
    "        k_hops_click = self.build_k_hop(click_docs)\n",
    "        \n",
    "        click_docs, log_mask = self.pad_to_fix_len(click_docs, self.user_log_length)\n",
    "        user_feature = self.news_combined[click_docs]\n",
    "\n",
    "        pos = self.trans_to_nindex(sess_pos)\n",
    "        neg = self.trans_to_nindex(sess_neg)\n",
    "\n",
    "        label = random.randint(0, self.npratio)\n",
    "        sample_news = neg[:label] + pos + neg[label:]\n",
    "        news_feature = self.news_combined[sample_news]\n",
    "        return k_hops_click, [torch.from_numpy(user_feature), torch.from_numpy(log_mask), \\\n",
    "        torch.from_numpy(news_feature), torch.tensor(label)]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        k_hops_click, dt =  self.preprocessDT[idx]\n",
    "        subemb = self.news_graph.x[k_hops_click]\n",
    "        sub_edge_index, sub_edge_attr = subgraph(k_hops_click, self.news_graph.edge_index, self.news_graph.edge_attr, \\\n",
    "                                                 relabel_nodes=True, num_nodes=self.news_graph.num_nodes)\n",
    "        sub_news_graph = Data(x=subemb, edge_index=sub_edge_index, edge_attr=sub_edge_attr).to(device)\n",
    "        return sub_news_graph, dt\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.preprocessDT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dddb5119-b88a-439a-b76c-3fe3729c1242",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10172it [00:01, 8236.42it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = Dataset_PANEL_1(\n",
    "                filename=target_file,\n",
    "                news_index=news_index,\n",
    "                news_combined=news_input,\n",
    "                cfg=cfg,\n",
    "                neighbor_dict=news_neighbors_dict,\n",
    "                news_graph=news_graph\n",
    ")\n",
    "dataloader = GraphDataLoader(dataset, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9505e17-0339-4677-9bcf-3b1fddee7b84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 100, 22])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iterator = iter(dataloader)\n",
    "data_batch = next(iterator)\n",
    "sub_news_graph, [user_feature, log_mask, news_feature, label] = data_batch\n",
    "user_feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ddb6e82-f786-4e59-947d-adfa3b7da10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max(sub_news_graph.batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0b77c2f-5b08-41da-886d-a4064931c6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GCNConv,GATv2Conv, GatedGraphConv\n",
    "from torch_geometric.nn import global_mean_pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "84502714-78b3-4e44-a5df-530935c54695",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionPooling(nn.Module):\n",
    "    def __init__(self, emb_size, hidden_size):\n",
    "        super(AttentionPooling, self).__init__()\n",
    "        self.att_fc1 = nn.Linear(emb_size, hidden_size)\n",
    "        self.att_fc2 = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x, attn_mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: batch_size, candidate_size, emb_dim\n",
    "            attn_mask: batch_size, candidate_size\n",
    "        Returns:\n",
    "            (shape) batch_size, emb_dim\n",
    "        \"\"\"\n",
    "        e = self.att_fc1(x)\n",
    "        e = nn.Tanh()(e)\n",
    "        alpha = self.att_fc2(e)\n",
    "        alpha = torch.exp(alpha)\n",
    "\n",
    "        if attn_mask is not None:\n",
    "            alpha = alpha * attn_mask.unsqueeze(2)\n",
    "\n",
    "        alpha = alpha / (torch.sum(alpha, dim=1, keepdim=True) + 1e-8)\n",
    "        if len(x.shape) == 3:\n",
    "            x = torch.bmm(x.permute(0, 2, 1), alpha).squeeze(dim=-1)\n",
    "        else:\n",
    "            x = torch.bmm(x.unsqueeze(-1), alpha.unsqueeze(-1)).squeeze(dim=-1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, d_k):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "        self.d_k = d_k\n",
    "\n",
    "    def forward(self, Q, K, V, attn_mask=None):\n",
    "        '''\n",
    "            Q: batch_size, n_head, candidate_num, d_k\n",
    "            K: batch_size, n_head, candidate_num, d_k\n",
    "            V: batch_size, n_head, candidate_num, d_v\n",
    "            attn_mask: batch_size, n_head, candidate_num\n",
    "            Return: batch_size, n_head, candidate_num, d_v\n",
    "        '''\n",
    "        scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(self.d_k)\n",
    "        scores = torch.exp(scores)\n",
    "\n",
    "        if attn_mask is not None:\n",
    "            scores = scores * attn_mask.unsqueeze(dim=-2)\n",
    "\n",
    "        attn = scores / (torch.sum(scores, dim=-1, keepdim=True) + 1e-8)\n",
    "        context = torch.matmul(attn, V)\n",
    "        return context\n",
    "\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_k, d_v):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "\n",
    "        self.W_Q = nn.Linear(d_model, d_k * n_heads)\n",
    "        self.W_K = nn.Linear(d_model, d_k * n_heads)\n",
    "        self.W_V = nn.Linear(d_model, d_v * n_heads)\n",
    "\n",
    "        self.scaled_dot_product_attn = ScaledDotProductAttention(self.d_k)\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight, gain=1)\n",
    "\n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        '''\n",
    "            Q: batch_size, candidate_num, d_model\n",
    "            K: batch_size, candidate_num, d_model\n",
    "            V: batch_size, candidate_num, d_model\n",
    "            mask: batch_size, candidate_num\n",
    "        '''\n",
    "        batch_size = Q.shape[0]\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(dim=1).expand(-1, self.n_heads, -1)\n",
    "\n",
    "        q_s = self.W_Q(Q).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        k_s = self.W_K(K).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        v_s = self.W_V(V).view(batch_size, -1, self.n_heads, self.d_v).transpose(1, 2)\n",
    "\n",
    "        context = self.scaled_dot_product_attn(q_s, k_s, v_s, mask)\n",
    "        output = context.transpose(1, 2).contiguous().view(batch_size, -1, self.n_heads * self.d_v)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e4eb076-b86c-4d29-997e-59ab12de9091",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsEncoder(nn.Module):\n",
    "    def __init__(self, embedding_matrix, num_category, num_subcategory):\n",
    "        super(NewsEncoder, self).__init__()\n",
    "        self.embedding_matrix = embedding_matrix\n",
    "        self.drop_rate = 0.2\n",
    "        self.num_words_title = 20\n",
    "        self.use_category = True\n",
    "        self.use_subcategory = True\n",
    "        category_emb_dim = 100\n",
    "        news_dim = 400\n",
    "        news_query_vector_dim = 200\n",
    "        word_embedding_dim = 300\n",
    "        self.category_emb = nn.Embedding(num_category + 1, category_emb_dim, padding_idx=0)\n",
    "        self.category_dense = nn.Linear(category_emb_dim, news_dim)\n",
    "        self.subcategory_emb = nn.Embedding(num_subcategory + 1, category_emb_dim, padding_idx=0)\n",
    "        self.subcategory_dense = nn.Linear(category_emb_dim, news_dim)\n",
    "        self.final_attn = AttentionPooling(news_dim, news_query_vector_dim)\n",
    "        self.cnn = nn.Conv1d(\n",
    "            in_channels=word_embedding_dim,\n",
    "            out_channels=news_dim,\n",
    "            kernel_size=3,\n",
    "            padding=1\n",
    "        )\n",
    "        self.attn = AttentionPooling(news_dim, news_query_vector_dim)\n",
    "        self.cln = nn.Linear(300,400)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        '''\n",
    "            x: batch_size, word_num\n",
    "            mask: batch_size, word_num\n",
    "        '''\n",
    "        title = torch.narrow(x, -1, 0, self.num_words_title).long()\n",
    "        word_vecs = F.dropout(self.embedding_matrix(title),\n",
    "                              p=self.drop_rate,\n",
    "                              training=self.training)\n",
    "        context_word_vecs = self.cnn(word_vecs.transpose(1, 2)).transpose(1, 2)\n",
    "        # context_word_vecs = self.cnn(word_vecs.transpose(1, 2)).transpose(1, 2)\n",
    "        # stop\n",
    "        title_vecs = self.attn(context_word_vecs, mask)\n",
    "        all_vecs = [title_vecs]\n",
    "\n",
    "        start = self.num_words_title\n",
    "        if self.use_category:\n",
    "            category = torch.narrow(x, -1, start, 1).squeeze(dim=-1).long()\n",
    "            category_vecs = self.category_dense(self.category_emb(category))\n",
    "            all_vecs.append(category_vecs)\n",
    "            start += 1\n",
    "        if self.use_subcategory:\n",
    "            subcategory = torch.narrow(x, -1, start, 1).squeeze(dim=-1).long()\n",
    "            subcategory_vecs = self.subcategory_dense(self.subcategory_emb(subcategory))\n",
    "            all_vecs.append(subcategory_vecs)\n",
    "\n",
    "        if len(all_vecs) == 1:\n",
    "            news_vecs = all_vecs[0]\n",
    "        else:\n",
    "            all_vecs = torch.stack(all_vecs, dim=1)\n",
    "            \n",
    "            news_vecs = self.final_attn(all_vecs)\n",
    "        return news_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f05fd2f-652d-4eaa-8da0-85f503ff1447",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UserEncoder, self).__init__()\n",
    "        news_dim = 400\n",
    "        user_query_vector_dim = 200\n",
    "        self.user_log_length = 100\n",
    "        self.user_log_mask = False\n",
    "        self.attn = AttentionPooling(news_dim, user_query_vector_dim)\n",
    "        self.pad_doc = nn.Parameter(torch.empty(1, news_dim).uniform_(-1, 1)).type(torch.FloatTensor)\n",
    "\n",
    "    def forward(self, news_vecs, log_mask=None):\n",
    "        '''\n",
    "            news_vecs: batch_size, history_num, news_dim\n",
    "            log_mask: batch_size, history_num\n",
    "        '''\n",
    "        bz = news_vecs.shape[0]\n",
    "        if self.user_log_mask:\n",
    "            user_vec = self.attn(news_vecs, log_mask)\n",
    "        else:\n",
    "            padding_doc = self.pad_doc.unsqueeze(dim=0).expand(bz, self.user_log_length, -1)\n",
    "            news_vecs = news_vecs * log_mask.unsqueeze(dim=-1) + padding_doc * (1 - log_mask.unsqueeze(dim=-1))\n",
    "            user_vec = self.attn(news_vecs)\n",
    "        return user_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ddaa5be1-8b93-41e0-a685-38c74b4ff8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NAML(torch.nn.Module):\n",
    "    def __init__(self, embedding_matrix, num_category, num_subcategory, **kwargs):\n",
    "        super(NAML, self).__init__()\n",
    "        pretrained_word_embedding = torch.from_numpy(embedding_matrix).float()\n",
    "        word_embedding = nn.Embedding.from_pretrained(pretrained_word_embedding,\n",
    "                                                      freeze=False,\n",
    "                                                      padding_idx=0)\n",
    "\n",
    "        self.news_encoder = NewsEncoder( word_embedding, num_category, num_subcategory)\n",
    "        self.user_encoder = UserEncoder()\n",
    "        self.news_dim = 400\n",
    "        self.gcn = GCNConv(400, 64)\n",
    "        self.gln = nn.Linear(64, 400)\n",
    "        self.attn = AttentionPooling(self.news_dim, 128)\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        self.npratio = 4\n",
    "        self.user_log_length = 100\n",
    "\n",
    "    def forward(self, graph_batch, history, history_mask, candidate, label):\n",
    "        '''\n",
    "            history: batch_size, history_length, num_word_title\n",
    "            history_mask: batch_size, history_length\n",
    "            candidate: batch_size, 1+K, num_word_title\n",
    "            label: batch_size, 1+K\n",
    "        '''\n",
    "        graph_vec, edge_index, batch = graph_batch.x, graph_batch.edge_index, graph_batch.batch\n",
    "        graph_vec = self.news_encoder(graph_vec)\n",
    "        graph_vec = self.gcn(graph_vec, edge_index)\n",
    "        graph_vec = graph_vec.relu()\n",
    "        graph_vec = self.gln(graph_vec)\n",
    "        graph_vec = global_mean_pool(graph_vec, batch)\n",
    "        \n",
    "        # graph_vec = F.dropout(graph_vec, p=0.2, training=self.training)\n",
    "        # graph_vec = self.gln(graph_vec)\n",
    "\n",
    "        num_words = history.shape[-1]\n",
    "        candidate_news = candidate.reshape(-1, num_words)\n",
    "        candidate_news_vecs = self.news_encoder(candidate_news).reshape(-1, 1 + self.npratio, self.news_dim)\n",
    "\n",
    "        \n",
    "        history_news = history.reshape(-1, num_words)\n",
    "        history_news_vecs = self.news_encoder(history_news).reshape(-1, self.user_log_length, self.news_dim)\n",
    "        user_vec = self.user_encoder(history_news_vecs, history_mask)\n",
    "\n",
    "        \n",
    "        # print(graph_vec.shape, user_vec.shape)\n",
    "        # stop\n",
    "        \n",
    "        uservec = torch.stack([user_vec, graph_vec], dim=1)\n",
    "        uservec = self.attn(uservec)\n",
    "\n",
    "        score = torch.bmm(candidate_news_vecs, user_vec.unsqueeze(dim=-1)).squeeze(dim=-1)\n",
    "        loss = self.loss_fn(score, label)\n",
    "        return loss, score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "42ab74c3-be64-4f83-856c-e0eab5720c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc(y_true, y_hat):\n",
    "    y_hat = torch.argmax(y_hat, dim=-1)\n",
    "    tot = y_true.shape[0]\n",
    "    hit = torch.sum(y_true == y_hat)\n",
    "    return hit.data.float() * 1.0 / tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "39f02757-1e70-4cd5-b950-5b41bdfa1b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------\n",
      "Dict length: 12506\n",
      "Have words: 11947\n",
      "Missing rate: 0.0446985446985447\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12506"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_dict = pickle.load(open(os.path.join(cfg.data_dir + '_train', \"category_dict.bin\"), \"rb\"))\n",
    "subcategory_dict = pickle.load(open(os.path.join(cfg.data_dir + '_train', \"subcategory_dict.bin\"), \"rb\"))\n",
    "word_dict = pickle.load(open(os.path.join(cfg.data_dir + '_train', \"word_dict.bin\"), \"rb\"))\n",
    "glove_emb = load_pretrain_emb(cfg.glove_path, word_dict, cfg.word_emb_dim)\n",
    "len(word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0b1fc4f7-bb86-403e-8582-da8bcf6ad281",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NAML(\n",
       "  (news_encoder): NewsEncoder(\n",
       "    (embedding_matrix): Embedding(12507, 300, padding_idx=0)\n",
       "    (category_emb): Embedding(18, 100, padding_idx=0)\n",
       "    (category_dense): Linear(in_features=100, out_features=400, bias=True)\n",
       "    (subcategory_emb): Embedding(265, 100, padding_idx=0)\n",
       "    (subcategory_dense): Linear(in_features=100, out_features=400, bias=True)\n",
       "    (final_attn): AttentionPooling(\n",
       "      (att_fc1): Linear(in_features=400, out_features=200, bias=True)\n",
       "      (att_fc2): Linear(in_features=200, out_features=1, bias=True)\n",
       "    )\n",
       "    (cnn): Conv1d(300, 400, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (attn): AttentionPooling(\n",
       "      (att_fc1): Linear(in_features=400, out_features=200, bias=True)\n",
       "      (att_fc2): Linear(in_features=200, out_features=1, bias=True)\n",
       "    )\n",
       "    (cln): Linear(in_features=300, out_features=400, bias=True)\n",
       "  )\n",
       "  (user_encoder): UserEncoder(\n",
       "    (attn): AttentionPooling(\n",
       "      (att_fc1): Linear(in_features=400, out_features=200, bias=True)\n",
       "      (att_fc2): Linear(in_features=200, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (gcn): GCNConv(400, 64)\n",
       "  (gln): Linear(in_features=64, out_features=400, bias=True)\n",
       "  (attn): AttentionPooling(\n",
       "    (att_fc1): Linear(in_features=400, out_features=128, bias=True)\n",
       "    (att_fc2): Linear(in_features=128, out_features=1, bias=True)\n",
       "  )\n",
       "  (loss_fn): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NAML(glove_emb, len(category_dict), len(subcategory_dict))\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0003)\n",
    "model = model.to(device)\n",
    "torch.set_grad_enabled(True)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "be1490b1-e367-4e07-a690-06f59425b44c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "79it [02:48,  2.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(124.7927, device='cuda:0') tensor(28.1769, device='cuda:0')\n",
      "EPOCH: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "79it [01:38,  1.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(112.9059, device='cuda:0') tensor(32.1305, device='cuda:0')\n",
      "EPOCH: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "79it [01:35,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(108.4390, device='cuda:0') tensor(34.9357, device='cuda:0')\n",
      "EPOCH: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "79it [01:36,  1.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(105.0765, device='cuda:0') tensor(36.4710, device='cuda:0')\n",
      "EPOCH: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "79it [01:37,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(101.5080, device='cuda:0') tensor(37.9835, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for ep in range(5):\n",
    "    loss = 0.0\n",
    "    accuary = 0.0\n",
    "    print(\"EPOCH: \" + str(ep))\n",
    "    for cnt, (g, [log_ids, log_mask, input_ids, targets]) in tqdm(enumerate(dataloader)):\n",
    "        log_ids = log_ids.to(device)\n",
    "        log_mask = log_mask.to(device)\n",
    "        input_ids = input_ids.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        bz_loss, y_hat = model(g, log_ids, log_mask, input_ids, targets)\n",
    "        loss += bz_loss.data.float()\n",
    "        accuary += acc(targets, y_hat)\n",
    "        optimizer.zero_grad()\n",
    "        bz_loss.backward()\n",
    "        optimizer.step()\n",
    "        # stop\n",
    "    torch.save(model.state_dict(), 'Graph_naml_model_ggcn.pth')\n",
    "    print(loss, accuary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9b94e2cf-c6b4-42fb-ba02-138e330e64b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), 'Graph_naml_model_gcn_full.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5b183612-de1e-4c6c-aa40-da8ecaceb5f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('Graph_naml_model_ggcn.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619ceb00-d40f-426c-94b3-3bd4646dcfa3",
   "metadata": {},
   "source": [
    "# Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "94ac9e3a-5b13-446f-bf6f-171c99d24cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d066192f-25d9-4a1a-b1c7-dab919a97df2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x207c931c070>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0c734545-520a-4773-90d7-dc0cfa911166",
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = \"val\"\n",
    "data_dir = {\"train\": cfg.data_dir + '_train', \"val\": cfg.data_dir + '_val', \"test\": cfg.data_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e48bd4e7-628e-4a12-8a7e-78b5328a9b9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./data/MINDsmall_val'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_index = pickle.load(open(Path(data_dir[mode]) / \"news_dict.bin\", \"rb\"))\n",
    "news_input = pickle.load(open(Path(data_dir[mode]) / \"nltk_token_news.bin\", \"rb\"))\n",
    "data_dir[mode]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "39596d22-f1bc-4945-a650-c38372afeb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_dataset = NewsDataset(news_input)\n",
    "news_dataloader = DataLoader(news_dataset, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03a3ce4-9bea-4357-ac10-841486f56bcf",
   "metadata": {},
   "source": [
    "## news -> scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9142bf3b-4d4e-4094-91bb-5a024a647abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 510/510 [00:01<00:00, 471.46it/s]\n"
     ]
    }
   ],
   "source": [
    "news_scoring = []\n",
    "with torch.no_grad():\n",
    "    for input_ids in tqdm(news_dataloader):\n",
    "        input_ids = input_ids.cuda()\n",
    "        news_vec = model.news_encoder(input_ids)\n",
    "        news_vec = news_vec.to(torch.device(\"cpu\")).detach().numpy()\n",
    "        news_scoring.extend(news_vec)\n",
    "\n",
    "news_scoring = np.array(news_scoring)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502b6dbe-19ba-40a6-89be-ab292c9c0791",
   "metadata": {},
   "source": [
    "## Val loader and compute score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8e664e0d-dae5-4cb9-b7f3-775b9939472f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValidDataset_PANEL_1(Dataset_PANEL_1):\n",
    "    def __init__(self, filename, news_index, news_score, cfg, neighbor_dict, news_graph):\n",
    "        super(Dataset_PANEL_1).__init__()\n",
    "        self.filename = filename\n",
    "        self.news_index = news_index\n",
    "        self.news_score = news_score\n",
    "        self.user_log_length = cfg.his_size\n",
    "        self.npratio = cfg.npratio\n",
    "        self.cfg = cfg\n",
    "        self.neighbor_dict = neighbor_dict\n",
    "        self.news_graph = news_graph\n",
    "        self.news_graph.x = self.news_graph.x.float()\n",
    "        self.prepare()\n",
    "        \n",
    "\n",
    "    def line_mapper(self, line):\n",
    "        line = line.strip().split('\\t')\n",
    "        click_docs = line[3].split()\n",
    "        \n",
    "        candidate_news = self.trans_to_nindex([i.split('-')[0] for i in line[4].split()])\n",
    "        label = np.array([int(i.split('-')[1]) for i in line[4].split()])\n",
    "        \n",
    "        click_docs = self.trans_to_nindex(click_docs)\n",
    "\n",
    "        # build sub-graph\n",
    "        k_hops_click = self.build_k_hop(click_docs)\n",
    "        \n",
    "        click_docs, log_mask = self.pad_to_fix_len(click_docs, self.user_log_length)\n",
    "        user_feature = self.news_score[click_docs]\n",
    "\n",
    "        news_feature = self.news_score[candidate_news]\n",
    "        \n",
    "        return k_hops_click,  [torch.from_numpy(user_feature), torch.from_numpy(log_mask), \\\n",
    "        torch.from_numpy(news_feature), torch.tensor(label)]\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        k_hops_click, dt =  self.preprocessDT[idx]\n",
    "        subemb = torch.from_numpy(self.news_score[k_hops_click])\n",
    "        sub_edge_index, sub_edge_attr = subgraph(k_hops_click, self.news_graph.edge_index, self.news_graph.edge_attr, \\\n",
    "                                                 relabel_nodes=True, num_nodes=self.news_graph.num_nodes)\n",
    "        sub_news_graph = Data(x=subemb, edge_index=sub_edge_index, edge_attr=sub_edge_attr).to(device)\n",
    "        return sub_news_graph, dt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "098ac2d6-3047-4e0d-8e5f-b33f0d19254e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('data/MINDsmall_val/behaviors.tsv')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_target_file = Path(data_dir[mode]) / f\"behaviors.tsv\"\n",
    "valid_target_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0cc6c9c4-9065-4ddb-98db-af4381ba7d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_graph = torch.load(Path(data_dir[mode]) / \"nltk_news_graph.pt\")\n",
    "news_neighbors_dict = pickle.load(open(Path(data_dir[mode]) / \"news_neighbor_dict.bin\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1774a9e9-5684-4d1e-8618-5e154dd20523",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10285it [00:02, 3571.38it/s]\n"
     ]
    }
   ],
   "source": [
    "valid_dataset = ValidDataset_PANEL_1(\n",
    "                filename=valid_target_file,\n",
    "                news_index=news_index,\n",
    "                news_score=news_scoring,\n",
    "                cfg=cfg,\n",
    "                neighbor_dict=news_neighbors_dict,\n",
    "                news_graph=news_graph\n",
    ")\n",
    "valid_dataloader = GraphDataLoader(valid_dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0298f963-fe21-4f84-9291-1495c24e3783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterator = iter(valid_dataloader)\n",
    "# data_batch = next(iterator)\n",
    "# g,[ uf, lm, nf, l] = data_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69776be-92cd-461e-af23-09bc502fcd34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "093e3b3e-214a-4958-a7bd-dbe9c8f4f5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def print_metrics(cnt, x):\n",
    "    print(cnt, x)\n",
    "\n",
    "def get_mean(arr):\n",
    "    return [np.array(i).mean() for i in arr]\n",
    "\n",
    "def get_sum(arr):\n",
    "    return [np.array(i).sum() for i in arr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c70af8bf-0d03-4900-bab4-7913cd5ab743",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16it [00:00, 80.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [0.19047619047619047, 0.05555555555555555, 0.0, 0.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10001it [01:45, 95.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 [0.6452614058447184, 0.30694968726462185, 0.3375410393158319, 0.397463076172307]\n",
      "10000 [0.6452614058447184, 0.30694968726462185, 0.3375410393158319, 0.397463076172307]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "AUC = []\n",
    "MRR = []\n",
    "nDCG5 = []\n",
    "nDCG10 = []\n",
    "\n",
    "for cnt, (graph_batch, [log_vecs, log_mask, news_vecs, labels]) in tqdm(enumerate(valid_dataloader)):\n",
    "    log_vecs = log_vecs.to(device)\n",
    "    log_mask = log_mask.to(device)\n",
    "\n",
    "    graph_vec, edge_index, batch = graph_batch.x, graph_batch.edge_index, graph_batch.batch\n",
    "    # graph_vec = model.news_encoder(graph_vec)\n",
    "    graph_vec = model.gcn(graph_vec, edge_index)\n",
    "    graph_vec = graph_vec.relu()\n",
    "    graph_vec = model.gln(graph_vec)\n",
    "    graph_vec = global_mean_pool(graph_vec, batch)\n",
    "\n",
    "    # user_vecs = model.user_encoder(log_vecs, log_mask).to(torch.device(\"cpu\")).detach().numpy()\n",
    "    # user_vecs = torch.cat((user_vecs, graph_vec), dim=1)\n",
    "    # user_vecs = model.attn(user_vecs)\n",
    "    # user_vecs = model.ln(user_vecs).to(torch.device(\"cpu\")).detach().numpy()\n",
    "    # news_vecs = news_vecs.to(torch.device(\"cpu\")).detach().numpy()\n",
    "\n",
    "    user_vecs = model.user_encoder(log_vecs, log_mask)\n",
    "    user_vecs = torch.stack([user_vecs, graph_vec], dim=1)\n",
    "    user_vecs = model.attn(user_vecs).to(torch.device(\"cpu\")).detach().numpy()\n",
    "    \n",
    "    labels = labels.to(torch.device(\"cpu\")).detach().numpy()\n",
    "    \n",
    "    for user_vec, news_vec, label in zip(user_vecs, news_vecs, labels):\n",
    "        tmp = np.mean(label)\n",
    "        if tmp == 0 or tmp == 1:\n",
    "            continue\n",
    "\n",
    "        score = np.dot(news_vec, user_vec)\n",
    "        auc = roc_auc_score(label, score)\n",
    "        mrr = mrr_score(label, score)\n",
    "        ndcg5 = ndcg_score(label, score, k=5)\n",
    "        ndcg10 = ndcg_score(label, score, k=10)\n",
    "\n",
    "        AUC.append(auc)\n",
    "        MRR.append(mrr)\n",
    "        nDCG5.append(ndcg5)\n",
    "        nDCG10.append(ndcg10)\n",
    "\n",
    "    if cnt % 10000 == 0:\n",
    "        print_metrics(cnt, get_mean([AUC, MRR, nDCG5, nDCG10]))\n",
    "\n",
    "print_metrics(cnt, get_mean([AUC, MRR, nDCG5, nDCG10]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
